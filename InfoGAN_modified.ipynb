{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 날짜 : 2022-01-19\n",
      "GPU device : cuda:2\n",
      "image.size() =  torch.Size([128, 1, 32, 32]) \ttype torch.FloatTensor\n",
      "label.size() =  torch.Size([128]) \ttype torch.LongTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "args = {\n",
    "        'GPU_NUM' : 2,\n",
    "        'Epochs' : 200,\n",
    "        'batch_size' : 128,\n",
    "        'lr' : 0.0002,\n",
    "        'b1' : 0.5,\n",
    "        'b2' : 0.999,\n",
    "        'latent_dim' : 62,\n",
    "        'code_dim' : 2,\n",
    "        'n_classes' : 2,\n",
    "        'img_size' : 32,\n",
    "        'channels' : 1,\n",
    "        'sample_interval' : 400\n",
    "        }\n",
    "\n",
    "device = torch.device('cuda:{}'.format(args['GPU_NUM']) if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "today = datetime.date.today()\n",
    "print('오늘 날짜 :',today)\n",
    "print('GPU device :', device)\n",
    "\n",
    "my_transform =transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                transforms.Resize(args['img_size']), \n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "train_data = ImageFolder('MNIST/classes/binary/train', transform = my_transform)\n",
    "test_data = ImageFolder('MNIST/classes/binary/test', transform = my_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=args['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=args['batch_size'], shuffle=True)\n",
    "\n",
    "pltsize = 1\n",
    "plt.figure(figsize=(10*pltsize,pltsize))\n",
    "for image, label in train_loader:\n",
    "    print('image.size() = ',image.size(), '\\ttype', image.type())\n",
    "    print('label.size() = ', label.size(), '\\ttype', label.type())\n",
    "    break\n",
    "\n",
    "plt.figure(figsize=(10*pltsize,pltsize))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image[i,:,:,:].permute(1,2,0), cmap=\"gray\")\n",
    "    plt.title('class '+ str(label[i].item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(tensor):\n",
    "    return tensor.cpu().detach().numpy()\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    \"\"\" Conv layer는 mean이 0, std가 0.02인 가우시안 분포로 weight init\n",
    "        BatchNorm은 mean이 1, std가 0.02인 가우시안 분포로 weight init\n",
    "        Bias term은 전부 0으로 초기화\n",
    "    Args:\n",
    "        m ([model]): 학습하려는 모델\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "def to_discrete(y, num_columns):\n",
    "    \"\"\" onehot encoding\n",
    "        (batch_size,)가 shape인 label이 있으면, (64,num_columns)인 zeros 행렬을 생성하고,\n",
    "        (batch_size,)의 label vector element 값의 index만 1로 바꿔서 one-hot encoding함\n",
    "    Args:\n",
    "        y : 어떤 array (y.shape[0]는 batch_size로 보면 됨)\n",
    "        num_columns : num_classes\n",
    "    \"\"\"\n",
    "    y_disc = np.zeros((y.shape[0], num_columns))\n",
    "    y_disc[range(y.shape[0]), y] = 1.0 # one-hot encoding()\n",
    "\n",
    "    return Variable(FloatTensor(y_disc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        #nn.Conv2d(input channel, output channel, ...)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes,\n",
    "                               kernel_size = 3,\n",
    "                               stride=stride,\n",
    "                               padding=1,\n",
    "                               bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes) # Batchnorm은 사이의 가중치가 아니라 출력 층만 노말라이징\n",
    "        self.conv2 = nn.Conv2d(planes, planes,\n",
    "                               kernel_size = 3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size = 1,\n",
    "                          stride=stride,\n",
    "                          bias=False),\n",
    "                nn.BatchNorm2d(planes))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ResNet_3232(nn.Module):\n",
    "    def __init__(self, channels = 1, num_classes = 10):\n",
    "        super(ResNet_3232, self).__init__()\n",
    "        \n",
    "        self.rgb = channels\n",
    "        self.in_planes = 16\n",
    "        # RGB여서 3, in_planes는 내맘대로 16\n",
    "        self.conv1 = nn.Conv2d(self.rgb,16,\n",
    "                               kernel_size = 3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=2)\n",
    "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
    "        self.linear = nn.Linear(64,2)\n",
    "        \n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] *(num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(ResidualBlock(self.in_planes,planes,stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        out2 = self.layer1(out1)\n",
    "        out3 = self.layer2(out2)\n",
    "        out4 = self.layer3(out3)\n",
    "        out5 = F.avg_pool2d(out4, 4)\n",
    "        out6 = out5.view(out5.size(0), -1)\n",
    "        out7 = self.linear(out6)\n",
    "        \n",
    "        return out7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval ==0:\n",
    "            print(\"Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                Epoch, batch_idx*len(image), len(train_loader.dataset), 100.*batch_idx/len(train_loader), loss.item()))\n",
    "    return output\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    correct=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1,keepdim=True)[1] # output에서 제일 큰 놈의 index를 반환한다(이경우에 0 or 1)\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100*correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNet_3232(channels = args['channels'], num_classes=args['n_classes']).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# best_accuracy = 0\n",
    "# for Epoch in range(1, 10+1):\n",
    "#     train(model, train_loader, optimizer, log_interval=200)\n",
    "#     test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "#     if test_accuracy > best_accuracy:\n",
    "#         best_accuracy = test_accuracy\n",
    "#         torch.save(model, 'pretrained_model/ResNet_3232_1_7.pt')\n",
    "#         torch.save(model.state_dict(), 'pretrained_model/ResNet_3232_parameters_1_7.pt')\n",
    "#     print(\"[EPOCH: {}], \\tTest Loss: {:.4f},\\tTest Accuracy: {:.2f}%\\n\".format(\n",
    "#         Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded output shape is (64, 4, 4)\n",
    "# continuous dim = 1 (가장 중요한 code 하나만 뽑아보자)\n",
    "# discrete dim = 2 (n_classes가 2)\n",
    "\n",
    "args['cont_dim_P'] = 1\n",
    "args['cont_dim_G'] = 2\n",
    "args['disc_dim'] = 2\n",
    "args['latent_dim'] = 32\n",
    "args['reduced_dim'] = args['cont_dim_P'] + args['cont_dim_G'] + args['latent_dim'] # 37\n",
    "\n",
    "\n",
    "class Mapper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mapper, self).__init__()\n",
    "        \n",
    "        self.reduce_layer = nn.Sequential(nn.Linear(64*4*4, args['reduced_dim']), nn.ReLU(),nn.BatchNorm1d(args['reduced_dim']))\n",
    "        \n",
    "        self.cont_layer_P = nn.Sequential(nn.Linear(args['reduced_dim'], args['cont_dim_P']), nn.ReLU(),nn.BatchNorm1d(args['cont_dim_P']))\n",
    "        self.cont_layer_G = nn.Sequential(nn.Linear(args['reduced_dim'], args['cont_dim_G']), nn.ReLU(),nn.BatchNorm1d(args['cont_dim_G']))\n",
    "        self.latent_layer = nn.Sequential(nn.Linear(args['reduced_dim'], args['latent_dim']), nn.ReLU(),nn.BatchNorm1d(args['latent_dim']))\n",
    "        \n",
    "    def forward(self, encoded):\n",
    "        encoded = encoded.view(-1,64*4*4)\n",
    "        reduced = self.reduce_layer(encoded)\n",
    "        cont_code_P = self.cont_layer_P(reduced)\n",
    "        cont_code_G = self.cont_layer_G(reduced)\n",
    "        latent = self.latent_layer(reduced)\n",
    "        return cont_code_P, cont_code_G, latent\n",
    "        \n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Predictor, self).__init__()\n",
    "            \n",
    "        self.input_dim = args['cont_dim_P']\n",
    "        self.predictor = nn.Sequential(nn.Linear(self.input_dim, 128),nn.ReLU(), nn.Dropout(0.2),\n",
    "                                        nn.Linear(128, 64), nn.ReLU(),nn.Dropout(0.2),\n",
    "                                        nn.Linear(64,2)) # predict class (1 or 7)\n",
    "    def forward(self, cont_code_P):\n",
    "        predicted = self.predictor(cont_code_P)\n",
    "        return predicted\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        input_dim = args['reduced_dim'] + args['n_classes'] # 32 + 2 + 2 + 1\n",
    "        \n",
    "        self.init_size = args['img_size'] // 4\n",
    "        # conv에 넣을 수 있도록 dim_adjust\n",
    "        self.fc = nn.Sequential(nn.Linear(input_dim, 128 * self.init_size * self.init_size ))\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128), nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128,128,3, stride=1, padding=1), nn.BatchNorm2d(128,0.8), nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128,64,3, stride=1, padding=1), nn.BatchNorm2d(64,0.8), nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64,args['channels'], 3, stride=1, padding=1), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, labels, cont_code_P, cont_code_G, latent):\n",
    "        gen_input = torch.cat((labels, cont_code_P, cont_code_G, latent), dim=-1) # cat => 32+2+2+1 = 37 \\\n",
    "        # print(gen_input.shape, args['reduced_dim'])\n",
    "        #  mat1 and mat2 shapes cannot be multiplied (128x37 and 36x8192)\n",
    "        out = self.fc(gen_input)    \n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size) # block화\n",
    "        img = self.conv_blocks(out)\n",
    "        \n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        def discriminator_block(in_channels, out_channels, bn=True):\n",
    "            block = [nn.Conv2d(in_channels, out_channels, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_channels, 0.8))\n",
    "            return block\n",
    "\n",
    "        # batchnorm을 넣을지 말지에 따라서 discriminator_block의 모듈 수가 달라진다\n",
    "        # 달라지면 nn.Sequential에 추가할 때 if로 나눠서 해야하나? 싶지만\n",
    "        # *를 사용하면 block안에 모듈이 몇개든 그냥 싹다 넣어주는 역할을 한다.\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            *discriminator_block(args['channels'], 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128)\n",
    "        )\n",
    "        \n",
    "        downsample_size = args['img_size'] // (2**4) #stride 2인 block이 4개 있었으니까 4번 downsampled \n",
    "        \n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128*downsample_size*downsample_size, 1)) # real or fake 예측 \n",
    "        self.disc_layer = nn.Sequential(nn.Linear(128*downsample_size*downsample_size, args['n_classes']),\n",
    "                                       nn.Softmax()) # class 예측\n",
    "        self.cont_P_layer = nn.Sequential(nn.Linear(128*downsample_size*downsample_size, \n",
    "                                                    args['cont_dim_P'])) # cont_code_P 예측\n",
    "        self.cont_G_layer = nn.Sequential(nn.Linear(128*downsample_size*downsample_size, \n",
    "                                                    args['cont_dim_G'])) # cont_code_G 예측\n",
    "        \n",
    "    def forward(self, img):\n",
    "        out = self.conv_blocks(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        reality = self.adv_layer(out)\n",
    "        pred_label = self.disc_layer(out)\n",
    "        pred_cont_P = self.cont_P_layer(out)\n",
    "        pred_cont_G = self.cont_G_layer(out)\n",
    "        \n",
    "        return reality, pred_label, pred_cont_P, pred_cont_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_loss = nn.CrossEntropyLoss().to(device)\n",
    "recon_loss = nn.MSELoss().to(device)\n",
    "adversarial_loss = nn.MSELoss().to(device)\n",
    "discrete_loss = nn.CrossEntropyLoss().to(device)\n",
    "continuous_loss = nn.MSELoss().to(device)\n",
    "\n",
    "lambda_disc = 1\n",
    "lambda_cont = 0.5\n",
    "\n",
    "pretrained_resnet = ResNet_3232(channels=1, num_classes=2).to(device)\n",
    "pretrained_resnet.load_state_dict(torch.load('pretrained_model/ResNet_3232_parameters_1_7.pt'))\n",
    "\n",
    "E = nn.Sequential(*(list(pretrained_resnet.children())[:5])).to(device)\n",
    "M = Mapper().to(device)\n",
    "P = Predictor().to(device)\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "\n",
    "M.apply(weights_init_normal)\n",
    "P.apply(weights_init_normal)\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "# os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST(\n",
    "#         root=\"../../data/mnist\",\n",
    "#         train=True,\n",
    "#         download=True,\n",
    "#         transform=transforms.Compose(\n",
    "#             [transforms.Resize(args['img_size']), \n",
    "#              transforms.ToTensor(), \n",
    "#              transforms.Normalize([0.5], [0.5])]\n",
    "#         ),\n",
    "#     ),\n",
    "#     batch_size=args['batch_size'],\n",
    "#     shuffle=True,\n",
    "# )\n",
    "\n",
    "optimizer_P = torch.optim.Adam(itertools.chain(M.parameters(),P.parameters()), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(M.parameters(),G.parameters()), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "optimizer_D = torch.optim.Adam(itertools.chain(M.parameters(),D.parameters()), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "optimizer_info = torch.optim.Adam(\n",
    "    itertools.chain(M.parameters(), P.parameters(), G.parameters(), D.parameters()), lr=args['lr'], betas=(args['b1'], args['b2'])\n",
    ")\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if (device == 'cuda') else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if (device == 'cuda') else torch.LongTensor\n",
    "\n",
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"noise에 의한 이미지 변화만 볼거라서 code는 0으로 다 고정\"\"\"\n",
    "    # 10*10 짜리 grid 만들려고 batch_size를 100으로 하는거임 => 10개 클래스 이미지를 10등분으로 interpolation\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, args['latent_dim']))))\n",
    "    static_label = to_discrete(np.array([num for _ in range(args.n_classes) for num in range(args['n_classes'])]), \n",
    "                               args['n_classes']) \n",
    "    static_code = Variable(FloatTensor(np.zeros((args['n_classes']**2, args['code_dim']))))\n",
    "    static_sample = G(z, static_label, static_code)\n",
    "    save_image(static_sample.data, \"images/static/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "\n",
    "    \"\"\"code의 interpolation에 의한 이미지 변화만 볼거라서 noise도 0으로 다 고정\"\"\"\n",
    "    static_z = Variable(FloatTensor(np.zeros((args['n_classes']**2, args['latent_dim']))))\n",
    "    # code 하나 고정용 벡터 생성\n",
    "    zeros = np.zeros((n_row ** 2, 1))\n",
    "    # n_row(10)번만큼 반복되는 interpolation 생성 ex) 2번 반복 [[-1],[0],[1]] => [[-1],[-1],[0],[0],[1],[1]]\n",
    "    c_varied = np.repeat(np.linspace(-1, 1, n_row)[:, np.newaxis], n_row, 0)\n",
    "    # code 하나는 0으로 고정하고 나머지 code 하나를 -1에서 1로 interpolation한 tensor 생성 (100,2)\n",
    "    c1 = Variable(FloatTensor(np.concatenate((c_varied, zeros), -1)))\n",
    "    c2 = Variable(FloatTensor(np.concatenate((zeros, c_varied), -1)))\n",
    "    # 0~9까지 class에서 하나씩만 뽑아(static_label) interpolation한 code로 image 생성\n",
    "    sample1 = G(static_z, static_label, c1)\n",
    "    sample2 = G(static_z, static_label, c2)\n",
    "    save_image(sample1.data, \"images/varying_c1/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "    save_image(sample2.data, \"images/varying_c2/%d.png\" % batches_done, nrow=n_row, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts!\n",
      "[Epoch 0/200] [P : 0.0755] [D : 0.0244] [G : 27.0259] [disc: 0.3238] [cont P: 0.0241] [cont G: 0.0246] [time: 9.7]\n",
      "[Epoch 1/200] [P : 0.0256] [D : 0.0242] [G : 12.1613] [disc: 0.3188] [cont P: 0.0163] [cont G: 0.0195] [time: 18.2]\n",
      "[Epoch 2/200] [P : 0.0169] [D : 0.0322] [G : 8.8012] [disc: 0.3203] [cont P: 0.0132] [cont G: 0.0191] [time: 26.7]\n",
      "[Epoch 3/200] [P : 0.0082] [D : 0.0715] [G : 7.1562] [disc: 0.3175] [cont P: 0.0147] [cont G: 0.0208] [time: 34.9]\n",
      "[Epoch 4/200] [P : 0.0039] [D : 0.2932] [G : 5.4456] [disc: 0.3192] [cont P: 0.0287] [cont G: 0.0295] [time: 43.5]\n",
      "[Epoch 5/200] [P : 0.0289] [D : 0.2017] [G : 6.1240] [disc: 0.3240] [cont P: 0.0112] [cont G: 0.0217] [time: 53.0]\n",
      "[Epoch 6/200] [P : 0.0027] [D : 0.1317] [G : 5.4141] [disc: 0.3141] [cont P: 0.0078] [cont G: 0.0100] [time: 61.5]\n",
      "[Epoch 7/200] [P : 0.0017] [D : 0.1543] [G : 5.4271] [disc: 0.3151] [cont P: 0.0146] [cont G: 0.0152] [time: 69.9]\n",
      "[Epoch 8/200] [P : 0.0055] [D : 0.1583] [G : 5.9760] [disc: 0.3147] [cont P: 0.0129] [cont G: 0.0152] [time: 79.7]\n",
      "[Epoch 9/200] [P : 0.0014] [D : 0.1366] [G : 5.4898] [disc: 0.3142] [cont P: 0.0101] [cont G: 0.0140] [time: 89.0]\n",
      "[Epoch 10/200] [P : 0.0060] [D : 0.1435] [G : 5.5730] [disc: 0.3148] [cont P: 0.0163] [cont G: 0.0162] [time: 97.2]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11121/2134389486.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils import tensorboard\n",
    "\n",
    "loss_writer = tensorboard.SummaryWriter('logs/MNIST/loss')\n",
    "image_writer = tensorboard.SummaryWriter('logs/MNIST/image')\n",
    "\n",
    "start = time.time() ; print('Training starts!')\n",
    "\n",
    "for epoch in range(args['Epochs']):\n",
    "    for i, (imgs,labels) in enumerate(train_loader):\n",
    "        batch_size = imgs.shape[0]\n",
    "        real = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False).to(device)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False).to(device)\n",
    "        \n",
    "        real_imgs = Variable(imgs.type(FloatTensor)).to(device)\n",
    "        real_labels = to_discrete(labels.numpy(), args['n_classes']).to(device)\n",
    "        labels = labels.to(device)\n",
    "        encoded = E(real_imgs)        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Predictor\n",
    "        # -----------------\n",
    "        optimizer_P.zero_grad()\n",
    "        cont_code_P, cont_code_G, latent = M(encoded.clone())\n",
    "        predicted= P(cont_code_P)\n",
    "        loss_P = predict_loss(predicted, real_labels)\n",
    "\n",
    "        loss_P.backward(retain_graph=True)\n",
    "        optimizer_P.step()\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "        cont_code_P, cont_code_G, latent = M(encoded.clone())\n",
    "        fake_imgs = G(real_labels, cont_code_P, cont_code_G, latent)\n",
    "        reality, _, _, _ = D(fake_imgs)\n",
    "        \n",
    "        loss_adv = adversarial_loss(reality, real) # fake_imgs의 분류(D) 결과가 최대한 1(real)로 분류되도록 G 학습\n",
    "        loss_recon = recon_loss(real_imgs, fake_imgs)\n",
    "        loss_G = loss_adv + 100*loss_recon\n",
    "        loss_G.backward(retain_graph=True)\n",
    "        optimizer_G.step()\n",
    "    \n",
    "        # -----------------\n",
    "        #  Train Discriminator\n",
    "        # -----------------\n",
    "        optimizer_D.zero_grad()\n",
    "        cont_code_P, cont_code_G, latent = M(encoded.clone())\n",
    "        # real or fake pred score\n",
    "        pred_real, _, _, _ = D(real_imgs)\n",
    "        pred_fake, _, _, _ = D(fake_imgs.detach())\n",
    "        loss_D_real = adversarial_loss(pred_real, real) # real_imgs는 D가 1(real)로 분류하도록 D 학습 \n",
    "        loss_D_fake = adversarial_loss(pred_fake, fake) # fake_imgs는 D가 0(fake)로 분류하도록 D 학습\n",
    "        loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "        \n",
    "        loss_D.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # ------------------\n",
    "        # Information Loss\n",
    "        # ------------------\n",
    "        optimizer_info.zero_grad()\n",
    "        cont_code_P, cont_code_G, latent = M(encoded.clone())\n",
    "        fake_imgs = G(real_labels, cont_code_P, cont_code_G, latent)\n",
    "        _, pred_label, pred_code_P, pred_code_G = D(fake_imgs) # D라고 해놨지만 Q head의 출력임\n",
    "        # pred_label = Variable(LongTensor(to_np(pred_label))).to(device)\n",
    "        # labels = Variable(LongTensor(to_np(labels))).to(device)\n",
    "        loss_info_disc = lambda_disc * discrete_loss(pred_label, real_labels) # 실제 레이블 예측(이산 CELoss)\n",
    "        loss_info_cont_P = lambda_cont * continuous_loss(pred_code_P, cont_code_P) # code_P_code 예측(연속 MSELoss)\n",
    "        loss_info_cont_G = lambda_cont * continuous_loss(pred_code_G, cont_code_G) # code_G_code 예측(연속 MSELoss)\n",
    "        loss_info =  loss_info_disc + loss_info_cont_P + loss_info_cont_G\n",
    "\n",
    "        loss_info.backward(retain_graph=True)\n",
    "        optimizer_info.step()\n",
    "        \n",
    "        # code만 고정된 경우 생성 image와 code만 interpolation하는 경우 생성 image 저장\n",
    "        # batches_done = epoch * len(train_loader) + i\n",
    "        # if batches_done % args['sample_interval'] == 0:\n",
    "        #     sample_image(n_row=2, batches_done=batches_done)\n",
    "            \n",
    "    # --------------\n",
    "    # Log Progress\n",
    "    # --------------\n",
    "    print(\n",
    "        \"[Epoch %d/%d] [P : %.4f] [D : %.4f] [G : %.4f] [disc: %.4f] [cont P: %.4f] [cont G: %.4f] [time: %.1f]\"\n",
    "        % (epoch, args['Epochs'], \n",
    "        #    i, len(train_loader),\n",
    "           loss_P.item(),\n",
    "           loss_D.item(), \n",
    "           loss_G.item(), \n",
    "           loss_info_disc.item(),\n",
    "           loss_info_cont_P.item(),\n",
    "           loss_info_cont_G.item(),\n",
    "           time.time()-start)\n",
    "    )\n",
    "    # loss_writer.add_scalar('loss_P', loss_P.item())\n",
    "    # loss_writer.add_scalar('loss_D', loss_D.item())\n",
    "    # loss_writer.add_scalar('loss_G', loss_G.item())\n",
    "    # loss_writer.add_scalar('loss_d', loss_info_disc.item())\n",
    "    # loss_writer.add_scalar('loss_c', loss_info_cont.item())\n",
    "    \n",
    "    # image_writer.add_image('static', , n_iter)\n",
    "    # image_writer.add_image('c_p_varied', ,n_iter)\n",
    "    # image_writer.add_image('c_g1_varied', ,n_iter)\n",
    "    # image_writer.add_image('c_g2_varied', ,n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "encoded = E(real_imgs)\n",
    "cont_code_P, cont_code_G, latent = M(encoded.clone())\n",
    "predicted= P(cont_code_P)\n",
    "fake_imgs = G(real_labels, cont_code_P, cont_code_G, latent)\n",
    "\n",
    "\n",
    "def get_image(n_row=7):\n",
    "    disc_zeros = np.zeros_like(to_np(disc_code[:,0][:,np.newaxis]))\n",
    "    disc_ones = np.ones_like(to_np(disc_code[:,0][:,np.newaxis]))\n",
    "    disc_c1 = Variable(LongTensor(np.concatenate((disc_ones, disc_zeros), -1))).to(device)\n",
    "    disc_c2 = Variable(LongTensor(np.concatenate((disc_zeros, disc_ones), -1))).to(device)\n",
    "    disc_c1_ones = G(cont_code_P, cont_code_G, disc_c1, latent)\n",
    "    disc_c2_ones = G(cont_code_P, cont_code_G, disc_c2, latent)\n",
    "    disc_c1_image = torchvision.utils.make_grid((disc_c1_ones[:n_row**2]),nrow=n_row)\n",
    "    disc_c2_image = torchvision.utils.make_grid((disc_c2_ones[:n_row**2]),nrow=n_row)\n",
    "    save_image(disc_c1_image,\"disc_c1_image.png\", normalize=True)\n",
    "    save_image(disc_c2_image,\"disc_c2_image.png\", normalize=True)\n",
    "    \n",
    "    \n",
    "    cont_zeros = np.zeros_like(to_np(cont_code_G[:,0][:,np.newaxis]))\n",
    "    cont_ones = np.ones_like(to_np(cont_code_G[:,0][:,np.newaxis]))\n",
    "    cont_g1 = Variable(FloatTensor(np.concatenate((cont_ones, disc_zeros), -1))).to(device)\n",
    "    cont_g2 = Variable(FloatTensor(np.concatenate((cont_zeros, disc_ones), -1))).to(device)\n",
    "    cont_g1_ones = G(cont_code_P, cont_g1, disc_code, latent)\n",
    "    cont_g2_ones = G(cont_code_P, cont_g2, disc_code, latent)\n",
    "    cont_g1_image = torchvision.utils.make_grid((cont_g1_ones[:n_row**2]),nrow=n_row)\n",
    "    cont_g2_image = torchvision.utils.make_grid((cont_g2_ones[:n_row**2]),nrow=n_row)\n",
    "\n",
    "    save_image(cont_g1_image, \"cont_g1_image.png\", normalize=True)\n",
    "    save_image(cont_g2_image, \"cont_g2_image.png\", normalize=True)\n",
    "    \n",
    "    cont_zeros_P = torch.zeros_like(cont_code_P)\n",
    "    cont_P_varied = FloatTensor(np.repeat(np.linspace(-1, 1, n_row)[:, np.newaxis], n_row, 0)).to(device)\n",
    "    c1_P_imgs = G(cont_P_varied, cont_code_G[:n_row**2], disc_c1[:n_row**2], latent[:n_row**2])\n",
    "    c2_P_imgs = G(cont_P_varied, cont_code_G[:n_row**2], disc_c2[:n_row**2], latent[:n_row**2])\n",
    "    P_imgs = torch.cat((c1_P_imgs, c2_P_imgs))\n",
    "    cont_P_varied = torchvision.utils.make_grid((P_imgs[:32]),nrow=2)\n",
    "    save_image(cont_P_varied,\"cont_P_varied.png\", normalize=True)\n",
    "\n",
    "get_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb97028dc70>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "IDX = 4\n",
    "\n",
    "plt.figure(figsize=(10,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image[IDX].permute(1,2,0).cpu().detach().numpy())\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(fake_imgs[IDX].permute(1,2,0).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted,_,_,_,_ = P(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted, cont_code_P, cont_code_G, disc_code, latent = P(encoded)\n",
    "fake = G(cont_code_P, cont_code_G, disc_code, latent)\n",
    "\n",
    "save_image(fake,\"apapap.png\", nrow=10, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row = 10\n",
    "zeros = np.zeros((n_row ** 2, 1))\n",
    "cont_P_varied = np.repeat(np.linspace(-2, 2, n_row)[:, np.newaxis], n_row, 0)\n",
    "cont_G_varied = np.repeat(np.linspace(-1, 1, n_row)[:, np.newaxis], n_row, 0)\n",
    "c1 = Variable(FloatTensor(np.concatenate((cont_G_varied, zeros), -1))).to(device)\n",
    "fake_p = G(FloatTensor(cont_P_varied).to(device), cont_code_G[:100], disc_code[:100], latent[:100])\n",
    "fake_g = G(cont_code_P[:100], c1.to(device), disc_code[:100], latent[:100])\n",
    "save_image(fake_g,\"varied_c2.png\", nrow=10, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.7153, device='cuda:2', grad_fn=<MinBackward1>) tensor(0.3685, device='cuda:2', grad_fn=<MaxBackward1>)\n",
      "tensor(-2.3641, device='cuda:2', grad_fn=<MinBackward1>) tensor(2.5974, device='cuda:2', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(cont_code_G), torch.max(cont_code_G))\n",
    "print(torch.min(cont_code_P), torch.max(cont_code_P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5974, device='cuda:2', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(cont_code_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"noise에 의한 이미지 변화만 볼거라서 code는 0으로 다 고정\"\"\"\n",
    "    # 10*10 짜리 grid 만들려고 batch_size를 100으로 하는거임 => 10개 클래스 이미지를 10등분으로 interpolation\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, args['latent_dim']))))\n",
    "    static_label = to_discrete(np.array([num for _ in range(args.n_classes) for num in range(args['n_classes'])]), \n",
    "                               args['n_classes']) \n",
    "    static_code = Variable(FloatTensor(np.zeros((args['n_classes']**2, args['code_dim']))))\n",
    "    static_sample = G(z, static_label, static_code)\n",
    "    save_image(static_sample.data, \"images/static/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "\n",
    "    \"\"\"code의 interpolation에 의한 이미지 변화만 볼거라서 noise도 0으로 다 고정\"\"\"\n",
    "    static_z = Variable(FloatTensor(np.zeros((args['n_classes']**2, args['latent_dim']))))\n",
    "    # code 하나 고정용 벡터 생성\n",
    "    zeros = np.zeros((n_row ** 2, 1))\n",
    "    # n_row(10)번만큼 반복되는 interpolation 생성 ex) 2번 반복 [[-1],[0],[1]] => [[-1],[-1],[0],[0],[1],[1]]\n",
    "    c_varied = np.repeat(np.linspace(-1, 1, n_row)[:, np.newaxis], n_row, 0)\n",
    "    # code 하나는 0으로 고정하고 나머지 code 하나를 -1에서 1로 interpolation한 tensor 생성 (100,2)\n",
    "    c1 = Variable(FloatTensor(np.concatenate((c_varied, zeros), -1)))\n",
    "    c2 = Variable(FloatTensor(np.concatenate((zeros, c_varied), -1)))\n",
    "    # 0~9까지 class에서 하나씩만 뽑아(static_label) interpolation한 code로 image 생성\n",
    "    sample1 = G(static_z, static_label, c1)\n",
    "    sample2 = G(static_z, static_label, c2)\n",
    "    save_image(sample1.data, \"images/varying_c1/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "    save_image(sample2.data, \"images/varying_c2/%d.png\" % batches_done, nrow=n_row, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11121/3330925285.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m a = torch.tensor([[1,0],\n\u001b[0m\u001b[1;32m      2\u001b[0m                   \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   [0,1]],dtype=torch.float32)\n\u001b[1;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,0],\n",
    "                  [1,0],\n",
    "                  [0,1]],dtype=torch.float32)\n",
    "b = torch.randn(a.shape) \n",
    "print(a)\n",
    "\n",
    "discrete_loss(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "997969026bb0563814df38f7ef8affc802fa04c571d985f23020a609d23c35a7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('opcode': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
