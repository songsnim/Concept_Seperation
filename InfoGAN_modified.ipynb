{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 날짜 : 2022-01-19\n",
      "GPU device : cuda:2\n",
      "image.size() =  torch.Size([128, 1, 32, 32]) \ttype torch.FloatTensor\n",
      "label.size() =  torch.Size([128]) \ttype torch.LongTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "args = {\n",
    "        'GPU_NUM' : 2,\n",
    "        'Epochs' : 200,\n",
    "        'batch_size' : 128,\n",
    "        'lr' : 0.0002,\n",
    "        'b1' : 0.5,\n",
    "        'b2' : 0.999,\n",
    "        'latent_dim' : 62,\n",
    "        'code_dim' : 2,\n",
    "        'n_classes' : 2,\n",
    "        'img_size' : 32,\n",
    "        'channels' : 1,\n",
    "        'sample_interval' : 400\n",
    "        }\n",
    "\n",
    "device = torch.device('cuda:{}'.format(args['GPU_NUM']) if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "today = datetime.date.today()\n",
    "print('오늘 날짜 :',today)\n",
    "print('GPU device :', device)\n",
    "\n",
    "my_transform =transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                transforms.Resize(args['img_size']), \n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "train_data = ImageFolder('MNIST/classes/binary/train', transform = my_transform)\n",
    "test_data = ImageFolder('MNIST/classes/binary/test', transform = my_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=args['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=args['batch_size'], shuffle=True)\n",
    "\n",
    "pltsize = 1\n",
    "plt.figure(figsize=(10*pltsize,pltsize))\n",
    "for image, label in train_loader:\n",
    "    print('image.size() = ',image.size(), '\\ttype', image.type())\n",
    "    print('label.size() = ', label.size(), '\\ttype', label.type())\n",
    "    break\n",
    "\n",
    "plt.figure(figsize=(10*pltsize,pltsize))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image[i,:,:,:].permute(1,2,0), cmap=\"gray\")\n",
    "    plt.title('class '+ str(label[i].item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(tensor):\n",
    "    return tensor.cpu().detach().numpy()\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    \"\"\" Conv layer는 mean이 0, std가 0.02인 가우시안 분포로 weight init\n",
    "        BatchNorm은 mean이 1, std가 0.02인 가우시안 분포로 weight init\n",
    "        Bias term은 전부 0으로 초기화\n",
    "    Args:\n",
    "        m ([model]): 학습하려는 모델\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "def to_discrete(y, num_columns):\n",
    "    \"\"\" onehot encoding\n",
    "        (batch_size,)가 shape인 label이 있으면, (64,num_columns)인 zeros 행렬을 생성하고,\n",
    "        (batch_size,)의 label vector element 값의 index만 1로 바꿔서 one-hot encoding함\n",
    "    Args:\n",
    "        y : 어떤 array (y.shape[0]는 batch_size로 보면 됨)\n",
    "        num_columns : num_classes\n",
    "    \"\"\"\n",
    "    y_disc = np.zeros((y.shape[0], num_columns))\n",
    "    y_disc[range(y.shape[0]), y] = 1.0 # one-hot encoding()\n",
    "\n",
    "    return Variable(FloatTensor(y_disc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        #nn.Conv2d(input channel, output channel, ...)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes,\n",
    "                               kernel_size = 3,\n",
    "                               stride=stride,\n",
    "                               padding=1,\n",
    "                               bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes) # Batchnorm은 사이의 가중치가 아니라 출력 층만 노말라이징\n",
    "        self.conv2 = nn.Conv2d(planes, planes,\n",
    "                               kernel_size = 3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size = 1,\n",
    "                          stride=stride,\n",
    "                          bias=False),\n",
    "                nn.BatchNorm2d(planes))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ResNet_3232(nn.Module):\n",
    "    def __init__(self, channels = 1, num_classes = 10):\n",
    "        super(ResNet_3232, self).__init__()\n",
    "        \n",
    "        self.rgb = channels\n",
    "        self.in_planes = 16\n",
    "        # RGB여서 3, in_planes는 내맘대로 16\n",
    "        self.conv1 = nn.Conv2d(self.rgb,16,\n",
    "                               kernel_size = 3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=2)\n",
    "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
    "        self.linear = nn.Linear(64,2)\n",
    "        \n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] *(num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(ResidualBlock(self.in_planes,planes,stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        out2 = self.layer1(out1)\n",
    "        out3 = self.layer2(out2)\n",
    "        out4 = self.layer3(out3)\n",
    "        out5 = F.avg_pool2d(out4, 4)\n",
    "        out6 = out5.view(out5.size(0), -1)\n",
    "        out7 = self.linear(out6)\n",
    "        \n",
    "        return out7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval ==0:\n",
    "            print(\"Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                Epoch, batch_idx*len(image), len(train_loader.dataset), 100.*batch_idx/len(train_loader), loss.item()))\n",
    "    return output\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    correct=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1,keepdim=True)[1] # output에서 제일 큰 놈의 index를 반환한다(이경우에 0 or 1)\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100*correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNet_3232(channels = args['channels'], num_classes=args['n_classes']).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# best_accuracy = 0\n",
    "# for Epoch in range(1, 10+1):\n",
    "#     train(model, train_loader, optimizer, log_interval=200)\n",
    "#     test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "#     if test_accuracy > best_accuracy:\n",
    "#         best_accuracy = test_accuracy\n",
    "#         torch.save(model, 'pretrained_model/ResNet_3232_1_7.pt')\n",
    "#         torch.save(model.state_dict(), 'pretrained_model/ResNet_3232_parameters_1_7.pt')\n",
    "#     print(\"[EPOCH: {}], \\tTest Loss: {:.4f},\\tTest Accuracy: {:.2f}%\\n\".format(\n",
    "#         Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded output shape is (64, 4, 4)\n",
    "# continuous dim = 1 (가장 중요한 code 하나만 뽑아보자)\n",
    "# discrete dim = 2 (n_classes가 2)\n",
    "\n",
    "args['cont_dim_P'] = 1\n",
    "args['cont_dim_G'] = 2\n",
    "args['disc_dim'] = 2\n",
    "args['latent_dim'] = 32\n",
    "args['reduced_dim'] = args['cont_dim_P'] + args['cont_dim_G'] + args['disc_dim'] + args['latent_dim'] # 37\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Predictor, self).__init__()\n",
    "            \n",
    "        self.input_dim = args['cont_dim_P']\n",
    "        self.predictor = nn.Sequential(nn.Linear(self.input_dim, 128),nn.ReLU(), nn.Dropout(0.2),\n",
    "                                        nn.Linear(128, 64), nn.ReLU(),nn.Dropout(0.2),\n",
    "                                        nn.Linear(64,2)) # predict class (1 or 7)\n",
    "        self.reduce_layer = nn.Linear(64*4*4, args['reduced_dim']) # 37 (total)\n",
    "        self.cont_layer_P = nn.Linear(args['reduced_dim'], args['cont_dim_P']) \n",
    "        self.cont_layer_G = nn.Linear(args['reduced_dim'], args['cont_dim_G'])\n",
    "        self.disc_layer = nn.Linear(args['reduced_dim'], args['disc_dim'], nn.Softmax())\n",
    "        self.latent_layer = nn.Linear(args['reduced_dim'], args['latent_dim'])\n",
    "        \n",
    "    def reduce_and_split_encoded(self, encoded):\n",
    "        encoded = encoded.view(-1, 64*4*4)\n",
    "        reduced = self.reduce_layer(encoded)\n",
    "        cont_code_P = self.cont_layer_P(reduced)\n",
    "        cont_code_G = self.cont_layer_G(reduced)\n",
    "        disc_code =  self.disc_layer(reduced)\n",
    "        latent = self.latent_layer(reduced)\n",
    "        return cont_code_P, cont_code_G, disc_code, latent\n",
    "            \n",
    "    def predict(self, cont_code_P):\n",
    "        predicted = self.predictor(cont_code_P)\n",
    "        return predicted\n",
    "        \n",
    "    def forward(self, encoded):\n",
    "        cont_code_P, cont_code_G, disc_code, latent = self.reduce_and_split_encoded(encoded)\n",
    "        predicted = self.predict(cont_code_P)\n",
    "        return predicted, cont_code_P, cont_code_G, disc_code, latent\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        input_dim = args['reduced_dim'] # 32 + 2 + 2 + 1\n",
    "        \n",
    "        self.init_size = args['img_size'] // 4\n",
    "        # conv에 넣을 수 있도록 dim_adjust\n",
    "        self.fc = nn.Sequential(nn.Linear(input_dim, 128 * self.init_size * self.init_size ))\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128), nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128,128,3, stride=1, padding=1), nn.BatchNorm2d(128,0.8), nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128,64,3, stride=1, padding=1), nn.BatchNorm2d(64,0.8), nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64,args['channels'], 3, stride=1, padding=1), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, cont_code_P, cont_code_G, disc_code, latent):\n",
    "        gen_input = torch.cat((cont_code_P, cont_code_G, disc_code, latent), dim=-1) # cat => 32+2+2+1 = 37 \\\n",
    "        # print(gen_input.shape, args['reduced_dim'])\n",
    "        #  mat1 and mat2 shapes cannot be multiplied (128x37 and 36x8192)\n",
    "        out = self.fc(gen_input)    \n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size) # block화\n",
    "        img = self.conv_blocks(out)\n",
    "        \n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        def discriminator_block(in_channels, out_channels, bn=True):\n",
    "            block = [nn.Conv2d(in_channels, out_channels, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_channels, 0.8))\n",
    "            return block\n",
    "\n",
    "        # batchnorm을 넣을지 말지에 따라서 discriminator_block의 모듈 수가 달라진다\n",
    "        # 달라지면 nn.Sequential에 추가할 때 if로 나눠서 해야하나? 싶지만\n",
    "        # *를 사용하면 block안에 모듈이 몇개든 그냥 싹다 넣어주는 역할을 한다.\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            *discriminator_block(args['channels'], 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128)\n",
    "        )\n",
    "        \n",
    "        downsample_size = args['img_size'] // (2**4) #stride 2인 block이 4개 있었으니까 4번 downsampled \n",
    "        \n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128*downsample_size*downsample_size, 1)) # real or fake 예측 \n",
    "        self.disc_layer = nn.Sequential(nn.Linear(128*downsample_size*downsample_size, args['disc_dim']),\n",
    "                                       nn.Softmax()) # class 예측 \n",
    "        self.cont_layer = nn.Sequential(nn.Linear(128*downsample_size*downsample_size, \n",
    "                                                    args['cont_dim_P']+args['cont_dim_G'])) # cont_code 예측\n",
    "        \n",
    "    def forward(self, img):\n",
    "        out = self.conv_blocks(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        reality = self.adv_layer(out)\n",
    "        disc_pred = self.disc_layer(out)\n",
    "        cont_pred = self.cont_layer(out)\n",
    "        \n",
    "        return reality, disc_pred, cont_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_loss = nn.CrossEntropyLoss().to(device)\n",
    "recon_loss = nn.MSELoss().to(device)\n",
    "adversarial_loss = nn.MSELoss().to(device)\n",
    "discrete_loss = nn.CrossEntropyLoss().to(device)\n",
    "continuous_loss = nn.MSELoss().to(device)\n",
    "\n",
    "lambda_disc = 1\n",
    "lambda_cont = 0.5\n",
    "\n",
    "pretrained_resnet = ResNet_3232(channels=1, num_classes=2).to(device)\n",
    "pretrained_resnet.load_state_dict(torch.load('pretrained_model/ResNet_3232_parameters_1_7.pt'))\n",
    "\n",
    "E = nn.Sequential(*(list(pretrained_resnet.children())[:5])).to(device)\n",
    "P = Predictor().to(device)\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "\n",
    "P.apply(weights_init_normal)\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "# os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST(\n",
    "#         root=\"../../data/mnist\",\n",
    "#         train=True,\n",
    "#         download=True,\n",
    "#         transform=transforms.Compose(\n",
    "#             [transforms.Resize(args['img_size']), \n",
    "#              transforms.ToTensor(), \n",
    "#              transforms.Normalize([0.5], [0.5])]\n",
    "#         ),\n",
    "#     ),\n",
    "#     batch_size=args['batch_size'],\n",
    "#     shuffle=True,\n",
    "# )\n",
    "\n",
    "optimizer_P = torch.optim.Adam(P.parameters(), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "optimizer_info = torch.optim.Adam(\n",
    "    itertools.chain(P.parameters(),G.parameters(), D.parameters()), lr=args['lr'], betas=(args['b1'], args['b2'])\n",
    ")\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if (device == 'cuda') else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if (device == 'cuda') else torch.LongTensor\n",
    "\n",
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"noise에 의한 이미지 변화만 볼거라서 code는 0으로 다 고정\"\"\"\n",
    "    # 10*10 짜리 grid 만들려고 batch_size를 100으로 하는거임 => 10개 클래스 이미지를 10등분으로 interpolation\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, args['latent_dim']))))\n",
    "    static_label = to_discrete(np.array([num for _ in range(args.n_classes) for num in range(args['n_classes'])]), \n",
    "                               args['n_classes']) \n",
    "    static_code = Variable(FloatTensor(np.zeros((args['n_classes']**2, args['code_dim']))))\n",
    "    static_sample = G(z, static_label, static_code)\n",
    "    save_image(static_sample.data, \"images/static/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "\n",
    "    \"\"\"code의 interpolation에 의한 이미지 변화만 볼거라서 noise도 0으로 다 고정\"\"\"\n",
    "    static_z = Variable(FloatTensor(np.zeros((args['n_classes']**2, args['latent_dim']))))\n",
    "    # code 하나 고정용 벡터 생성\n",
    "    zeros = np.zeros((n_row ** 2, 1))\n",
    "    # n_row(10)번만큼 반복되는 interpolation 생성 ex) 2번 반복 [[-1],[0],[1]] => [[-1],[-1],[0],[0],[1],[1]]\n",
    "    c_varied = np.repeat(np.linspace(-1, 1, n_row)[:, np.newaxis], n_row, 0)\n",
    "    # code 하나는 0으로 고정하고 나머지 code 하나를 -1에서 1로 interpolation한 tensor 생성 (100,2)\n",
    "    c1 = Variable(FloatTensor(np.concatenate((c_varied, zeros), -1)))\n",
    "    c2 = Variable(FloatTensor(np.concatenate((zeros, c_varied), -1)))\n",
    "    # 0~9까지 class에서 하나씩만 뽑아(static_label) interpolation한 code로 image 생성\n",
    "    sample1 = G(static_z, static_label, c1)\n",
    "    sample2 = G(static_z, static_label, c2)\n",
    "    save_image(sample1.data, \"images/varying_c1/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "    save_image(sample2.data, \"images/varying_c2/%d.png\" % batches_done, nrow=n_row, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x16 and 1024x37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11121/3005000175.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11121/944001701.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoded)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mcont_code_P\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_code_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_and_split_encoded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont_code_P\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_code_P\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_code_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11121/944001701.py\u001b[0m in \u001b[0;36mreduce_and_split_encoded\u001b[0;34m(self, encoded)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduce_and_split_encoded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mreduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mcont_code_P\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcont_layer_P\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mcont_code_G\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcont_layer_G\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x16 and 1024x37)"
     ]
    }
   ],
   "source": [
    "P(encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb97d63ac10>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(to_np(fake_imgs[1].permute(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "encoded = E(real_imgs)\n",
    "predicted, cont_code_P, cont_code_G, disc_code, latent = P(encoded)\n",
    "fake_imgs = G(cont_code_P, cont_code_G, disc_code, latent)\n",
    "\n",
    "\n",
    "def get_image(n_row=7):\n",
    "    disc_zeros = np.zeros_like(to_np(disc_code[:,0][:,np.newaxis]))\n",
    "    disc_ones = np.ones_like(to_np(disc_code[:,0][:,np.newaxis]))\n",
    "    disc_c1 = Variable(LongTensor(np.concatenate((disc_ones, disc_zeros), -1))).to(device)\n",
    "    disc_c2 = Variable(LongTensor(np.concatenate((disc_zeros, disc_ones), -1))).to(device)\n",
    "    disc_c1_ones = G(cont_code_P, cont_code_G, disc_c1, latent)\n",
    "    disc_c2_ones = G(cont_code_P, cont_code_G, disc_c2, latent)\n",
    "    disc_c1_image = torchvision.utils.make_grid((disc_c1_ones[:n_row**2]),nrow=n_row)\n",
    "    disc_c2_image = torchvision.utils.make_grid((disc_c2_ones[:n_row**2]),nrow=n_row)\n",
    "    save_image(disc_c1_image,\"disc_c1_image.png\", normalize=True)\n",
    "    save_image(disc_c2_image,\"disc_c2_image.png\", normalize=True)\n",
    "    \n",
    "    \n",
    "    cont_zeros = np.zeros_like(to_np(cont_code_G[:,0][:,np.newaxis]))\n",
    "    cont_ones = np.ones_like(to_np(cont_code_G[:,0][:,np.newaxis]))\n",
    "    cont_g1 = Variable(FloatTensor(np.concatenate((cont_ones, disc_zeros), -1))).to(device)\n",
    "    cont_g2 = Variable(FloatTensor(np.concatenate((cont_zeros, disc_ones), -1))).to(device)\n",
    "    cont_g1_ones = G(cont_code_P, cont_g1, disc_code, latent)\n",
    "    cont_g2_ones = G(cont_code_P, cont_g2, disc_code, latent)\n",
    "    cont_g1_image = torchvision.utils.make_grid((cont_g1_ones[:n_row**2]),nrow=n_row)\n",
    "    cont_g2_image = torchvision.utils.make_grid((cont_g2_ones[:n_row**2]),nrow=n_row)\n",
    "\n",
    "    save_image(cont_g1_image, \"cont_g1_image.png\", normalize=True)\n",
    "    save_image(cont_g2_image, \"cont_g2_image.png\", normalize=True)\n",
    "    \n",
    "    cont_zeros_P = torch.zeros_like(cont_code_P)\n",
    "    cont_P_varied = FloatTensor(np.repeat(np.linspace(-1, 1, n_row)[:, np.newaxis], n_row, 0)).to(device)\n",
    "    c1_P_imgs = G(cont_P_varied, cont_code_G[:n_row**2], disc_c1[:n_row**2], latent[:n_row**2])\n",
    "    c2_P_imgs = G(cont_P_varied, cont_code_G[:n_row**2], disc_c2[:n_row**2], latent[:n_row**2])\n",
    "    P_imgs = torch.cat((c1_P_imgs, c2_P_imgs))\n",
    "    cont_P_varied = torchvision.utils.make_grid((P_imgs[:32]),nrow=2)\n",
    "    save_image(cont_P_varied,\"cont_P_varied.png\", normalize=True)\n",
    "\n",
    "get_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([98, 1, 32, 32])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts!\n",
      "[Epoch 0/200] [P loss: 0.0225] [D loss: 0.0215] [G loss: 42.6909] [info disc: 0.7468] [info cont: 0.1034] [time: 8.7]\n",
      "[Epoch 1/200] [P loss: 0.0050] [D loss: 0.0119] [G loss: 14.1765] [info disc: 0.7240] [info cont: 0.1054] [time: 16.2]\n",
      "[Epoch 2/200] [P loss: 0.0044] [D loss: 0.0450] [G loss: 11.6901] [info disc: 0.7293] [info cont: 0.0538] [time: 24.7]\n",
      "[Epoch 3/200] [P loss: 0.0024] [D loss: 0.1378] [G loss: 11.6668] [info disc: 0.7066] [info cont: 0.0289] [time: 32.4]\n",
      "[Epoch 4/200] [P loss: 0.0051] [D loss: 0.0882] [G loss: 11.4432] [info disc: 0.7228] [info cont: 0.0289] [time: 39.8]\n",
      "[Epoch 5/200] [P loss: 0.0010] [D loss: 0.0725] [G loss: 10.8460] [info disc: 0.7317] [info cont: 0.0994] [time: 47.4]\n",
      "[Epoch 6/200] [P loss: 0.0006] [D loss: 0.0424] [G loss: 10.2126] [info disc: 0.7106] [info cont: 0.0907] [time: 55.5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11121/766334954.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/song/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils import tensorboard\n",
    "\n",
    "loss_writer = tensorboard.SummaryWriter('logs/MNIST/loss')\n",
    "image_writer = tensorboard.SummaryWriter('logs/MNIST/image')\n",
    "\n",
    "start = time.time() ; print('Training starts!')\n",
    "\n",
    "for epoch in range(args['Epochs']):\n",
    "    for i, (imgs,labels) in enumerate(train_loader):\n",
    "        batch_size = imgs.shape[0]\n",
    "        real = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False).to(device)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False).to(device)\n",
    "        \n",
    "        real_imgs = Variable(imgs.type(FloatTensor)).to(device)\n",
    "        labels = to_discrete(labels.numpy(), args['n_classes']).to(device)\n",
    "        \n",
    "        encoded = E(real_imgs)\n",
    "        \n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Predictor\n",
    "        # -----------------\n",
    "        optimizer_P.zero_grad()\n",
    "        \n",
    "        predicted, _, _, _, _ = P(encoded)\n",
    "        loss_P = predict_loss(predicted, labels)\n",
    "\n",
    "        loss_P.backward(retain_graph=True)\n",
    "        optimizer_P.step()\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator (input = latent, label, code)\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        \n",
    "        # z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, args['latent_dim']))))\n",
    "        # label_input = to_discrete(np.random.randint(0, args['n_classes'], batch_size), args['n_classes'])\n",
    "        # code_input = Variable(FloatTensor(np.random.uniform(-1,1, (batch_size, args['cont_dim']))))\n",
    "        _, cont_code_P, cont_code_G, disc_code, latent = P(encoded)\n",
    "        fake_imgs = G(cont_code_P, cont_code_G, disc_code, latent)\n",
    "        reality, _, _ = D(fake_imgs)\n",
    "        \n",
    "        loss_adv = adversarial_loss(reality, real) # fake_imgs의 분류(D) 결과가 최대한 1(real)로 분류되도록 G 학습\n",
    "        loss_recon = recon_loss(real_imgs, fake_imgs)\n",
    "        loss_G = loss_adv + 100*loss_recon\n",
    "        loss_G.backward(retain_graph=True)\n",
    "        optimizer_G.step()\n",
    "    \n",
    "        # -----------------\n",
    "        #  Train Discriminator\n",
    "        # -----------------\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # real or fake pred score\n",
    "        pred_real, _, _ = D(real_imgs)\n",
    "        pred_fake, _, _ = D(fake_imgs.detach())\n",
    "        loss_D_real = adversarial_loss(pred_real, real) # real_imgs는 D가 1(real)로 분류하도록 D 학습 \n",
    "        loss_D_fake = adversarial_loss(pred_fake, fake) # fake_imgs는 D가 0(fake)로 분류하도록 D 학습\n",
    "        loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "        \n",
    "        loss_D.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # ------------------\n",
    "        # Information Loss\n",
    "        # ------------------\n",
    "        optimizer_info.zero_grad()\n",
    "        \n",
    "        # sampled_labels = labels\n",
    "        \n",
    "        gt_labels = Variable(LongTensor(to_np(disc_code)), requires_grad=False).to(device)\n",
    "        \n",
    "        # z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, args['latent_dim']))))\n",
    "        \n",
    "        label_input = to_discrete(sampled_labels, args['n_classes'])\n",
    "        # code_input = Variable(FloatTensor(np.random.uniform(-1, 1, (batch_size, args['cont_dim']))))\n",
    "        \n",
    "        fake_imgs = G(cont_code_P, cont_code_G, disc_code, latent)\n",
    "        _, pred_label, pred_code = D(fake_imgs) # D라고 해놨지만 Q head의 출력임\n",
    "        cont_code = torch.cat((cont_code_P, cont_code_G), dim=-1)\n",
    "        # pred_label = Variable(LongTensor(to_np(pred_label))).to(device)\n",
    "        # labels = Variable(LongTensor(to_np(labels))).to(device)\n",
    "        loss_info_disc = lambda_disc * discrete_loss(pred_label, labels) # 실제 레이블 예측(이산 CELoss)\n",
    "        loss_info_cont = lambda_cont * continuous_loss(pred_code, cont_code)# code input 예측(연속 MSELoss)\n",
    "        loss_info =  loss_info_cont\n",
    "                    \n",
    "        loss_info.backward()\n",
    "        optimizer_info.step()\n",
    "        \n",
    "        # code만 고정된 경우 생성 image와 code만 interpolation하는 경우 생성 image 저장\n",
    "        # batches_done = epoch * len(train_loader) + i\n",
    "        # if batches_done % args['sample_interval'] == 0:\n",
    "        #     sample_image(n_row=2, batches_done=batches_done)\n",
    "            \n",
    "    # --------------\n",
    "    # Log Progress\n",
    "    # --------------\n",
    "    print(\n",
    "        \"[Epoch %d/%d] [P loss: %.4f] [D loss: %.4f] [G loss: %.4f] [info disc: %.4f] [info cont: %.4f] [time: %.1f]\"\n",
    "        % (epoch, args['Epochs'], \n",
    "        #    i, len(train_loader),\n",
    "           loss_P.item(),\n",
    "           loss_D.item(), \n",
    "           loss_G.item(), \n",
    "           loss_info_disc.item(),\n",
    "           loss_info_cont.item(),\n",
    "           time.time()-start)\n",
    "    )\n",
    "    loss_writer.add_scalar('loss_P', loss_P.item())\n",
    "    loss_writer.add_scalar('loss_D', loss_D.item())\n",
    "    loss_writer.add_scalar('loss_G', loss_G.item())\n",
    "    loss_writer.add_scalar('loss_d', loss_info_disc.item())\n",
    "    loss_writer.add_scalar('loss_c', loss_info_cont.item())\n",
    "    \n",
    "    image_writer.add_image('static', , n_iter)\n",
    "    image_writer.add_image('c_p_varied', ,n_iter)\n",
    "    image_writer.add_image('c_g1_varied', ,n_iter)\n",
    "    image_writer.add_image('c_g2_varied', ,n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5464, 0.4536], device='cuda:2', grad_fn=<SelectBackward0>)\n",
      "tensor([-5,  1], device='cuda:2')\n",
      "tensor([ 2.9278, -0.1782, -0.2597], device='cuda:2', grad_fn=<SelectBackward0>)\n",
      "tensor([ 2.2646, -0.2886, -0.2928], device='cuda:2', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(pred_label[0])\n",
    "print(gt_labels[0])\n",
    "\n",
    "print(pred_code[0])\n",
    "print(cont_code[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iamge, label in test_loader:\n",
    "    image = image.to(device)\n",
    "    encoded = E(image)\n",
    "    predicted, cont_code_P, cont_code_G, disc_code, latent = P(encoded)\n",
    "    fake_imgs = G(cont_code_P, cont_code_G, disc_code, latent)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb98937b130>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "IDX = 4\n",
    "\n",
    "plt.figure(figsize=(10,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image[IDX].permute(1,2,0).cpu().detach().numpy())\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(fake_imgs[IDX].permute(1,2,0).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted,_,_,_,_ = P(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted, cont_code_P, cont_code_G, disc_code, latent = P(encoded)\n",
    "fake = G(cont_code_P, cont_code_G, disc_code, latent)\n",
    "\n",
    "save_image(fake,\"apapap.png\", nrow=10, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row = 10\n",
    "zeros = np.zeros((n_row ** 2, 1))\n",
    "cont_P_varied = np.repeat(np.linspace(-2, 2, n_row)[:, np.newaxis], n_row, 0)\n",
    "cont_G_varied = np.repeat(np.linspace(-1, 1, n_row)[:, np.newaxis], n_row, 0)\n",
    "c1 = Variable(FloatTensor(np.concatenate((cont_G_varied, zeros), -1))).to(device)\n",
    "fake_p = G(FloatTensor(cont_P_varied).to(device), cont_code_G[:100], disc_code[:100], latent[:100])\n",
    "fake_g = G(cont_code_P[:100], c1.to(device), disc_code[:100], latent[:100])\n",
    "save_image(fake_g,\"varied_c2.png\", nrow=10, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.7153, device='cuda:2', grad_fn=<MinBackward1>) tensor(0.3685, device='cuda:2', grad_fn=<MaxBackward1>)\n",
      "tensor(-2.3641, device='cuda:2', grad_fn=<MinBackward1>) tensor(2.5974, device='cuda:2', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(cont_code_G), torch.max(cont_code_G))\n",
    "print(torch.min(cont_code_P), torch.max(cont_code_P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5974, device='cuda:2', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(cont_code_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"noise에 의한 이미지 변화만 볼거라서 code는 0으로 다 고정\"\"\"\n",
    "    # 10*10 짜리 grid 만들려고 batch_size를 100으로 하는거임 => 10개 클래스 이미지를 10등분으로 interpolation\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, args['latent_dim']))))\n",
    "    static_label = to_discrete(np.array([num for _ in range(args.n_classes) for num in range(args['n_classes'])]), \n",
    "                               args['n_classes']) \n",
    "    static_code = Variable(FloatTensor(np.zeros((args['n_classes']**2, args['code_dim']))))\n",
    "    static_sample = G(z, static_label, static_code)\n",
    "    save_image(static_sample.data, \"images/static/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "\n",
    "    \"\"\"code의 interpolation에 의한 이미지 변화만 볼거라서 noise도 0으로 다 고정\"\"\"\n",
    "    static_z = Variable(FloatTensor(np.zeros((args['n_classes']**2, args['latent_dim']))))\n",
    "    # code 하나 고정용 벡터 생성\n",
    "    zeros = np.zeros((n_row ** 2, 1))\n",
    "    # n_row(10)번만큼 반복되는 interpolation 생성 ex) 2번 반복 [[-1],[0],[1]] => [[-1],[-1],[0],[0],[1],[1]]\n",
    "    c_varied = np.repeat(np.linspace(-1, 1, n_row)[:, np.newaxis], n_row, 0)\n",
    "    # code 하나는 0으로 고정하고 나머지 code 하나를 -1에서 1로 interpolation한 tensor 생성 (100,2)\n",
    "    c1 = Variable(FloatTensor(np.concatenate((c_varied, zeros), -1)))\n",
    "    c2 = Variable(FloatTensor(np.concatenate((zeros, c_varied), -1)))\n",
    "    # 0~9까지 class에서 하나씩만 뽑아(static_label) interpolation한 code로 image 생성\n",
    "    sample1 = G(static_z, static_label, c1)\n",
    "    sample2 = G(static_z, static_label, c2)\n",
    "    save_image(sample1.data, \"images/varying_c1/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "    save_image(sample2.data, \"images/varying_c2/%d.png\" % batches_done, nrow=n_row, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11121/3330925285.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m a = torch.tensor([[1,0],\n\u001b[0m\u001b[1;32m      2\u001b[0m                   \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   [0,1]],dtype=torch.float32)\n\u001b[1;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,0],\n",
    "                  [1,0],\n",
    "                  [0,1]],dtype=torch.float32)\n",
    "b = torch.randn(a.shape) \n",
    "print(a)\n",
    "\n",
    "discrete_loss(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "997969026bb0563814df38f7ef8affc802fa04c571d985f23020a609d23c35a7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('opcode': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
