{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 날짜 : 2022-02-10\n",
      "GPU device : cuda:2\n",
      "image.size() =  torch.Size([128, 1, 32, 32]) \ttype torch.FloatTensor\n",
      "label.size() =  torch.Size([128]) \ttype torch.LongTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "args = {\n",
    "        'GPU_NUM' : 2,\n",
    "        'Epochs' : 200,\n",
    "        'batch_size' : 128,\n",
    "        'lr' : 0.0002,\n",
    "        'b1' : 0.5,\n",
    "        'b2' : 0.999,\n",
    "        'latent_dim' : 62,\n",
    "        'code_dim' : 2,\n",
    "        'n_classes' : 2,\n",
    "        'img_size' : 32,\n",
    "        'channels' : 1,\n",
    "        'sample_interval' : 400\n",
    "        }\n",
    "\n",
    "device = torch.device('cuda:{}'.format(args['GPU_NUM']) if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "today = datetime.date.today()\n",
    "print('오늘 날짜 :',today)\n",
    "print('GPU device :', device)\n",
    "\n",
    "my_transform =transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                transforms.Resize(args['img_size']), \n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "train_data = ImageFolder('MNIST/classes/binary/train', transform = my_transform)\n",
    "test_data = ImageFolder('MNIST/classes/binary/test', transform = my_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=args['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=args['batch_size'], shuffle=True)\n",
    "\n",
    "pltsize = 1\n",
    "plt.figure(figsize=(10*pltsize,pltsize))\n",
    "for image, label in train_loader:\n",
    "    print('image.size() = ',image.size(), '\\ttype', image.type())\n",
    "    print('label.size() = ', label.size(), '\\ttype', label.type())\n",
    "    break\n",
    "\n",
    "plt.figure(figsize=(10*pltsize,pltsize))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image[i,:,:,:].permute(1,2,0), cmap=\"gray\")\n",
    "    plt.title('class '+ str(label[i].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(tensor):\n",
    "    return tensor.cpu().detach().numpy()\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    \"\"\" Conv layer는 mean이 0, std가 0.02인 가우시안 분포로 weight init\n",
    "        BatchNorm은 mean이 1, std가 0.02인 가우시안 분포로 weight init\n",
    "        Bias term은 전부 0으로 초기화\n",
    "    Args:\n",
    "        m ([model]): 학습하려는 모델\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "def to_discrete(y, num_columns):\n",
    "    \"\"\" onehot encoding\n",
    "        (batch_size,)가 shape인 label이 있으면, (64,num_columns)인 zeros 행렬을 생성하고,\n",
    "        (batch_size,)의 label vector element 값의 index만 1로 바꿔서 one-hot encoding함\n",
    "    Args:\n",
    "        y : 어떤 array (y.shape[0]는 batch_size로 보면 됨)\n",
    "        num_columns : num_classes\n",
    "    \"\"\"\n",
    "    y_disc = np.zeros((y.shape[0], num_columns))\n",
    "    y_disc[range(y.shape[0]), y] = 1.0 # one-hot encoding()\n",
    "\n",
    "    return Variable(FloatTensor(y_disc))\n",
    "\n",
    "def sample_image(epoch):\n",
    "    folder_path = datetime.date.today()\n",
    "    os.makedirs('samples/{}_{}'.format(folder_path,args['description']), exist_ok=True)\n",
    "\n",
    "    inter = torch.linspace(-2,2,10).unsqueeze(1).to(device)\n",
    "   \n",
    "    # one !!\n",
    "    true_p, true_g, latent = M(E(image_one))\n",
    "    inter_p = inter\n",
    "    inter_g0 = torch.cat([inter, true_g[0, 1].unsqueeze(0).repeat(10,1)], dim=-1).to(device)\n",
    "    inter_g1 = torch.cat([true_g[0, 0].unsqueeze(0).repeat(10,1), inter], dim=-1).to(device)\n",
    "    label_one = torch.tensor([1,0], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    one_repeat = label_one.repeat(10,1).to(device)\n",
    "    one_latent_repeat = latent.repeat(10,1).to(device)\n",
    "    one_p = G(one_repeat, inter_p, true_g.repeat(10,1), one_latent_repeat)\n",
    "    one_g1 = G(one_repeat, true_p.repeat(10,1), inter_g0, one_latent_repeat)\n",
    "    one_g2 = G(one_repeat, true_p.repeat(10,1), inter_g1, one_latent_repeat)\n",
    "    sample_one = torch.cat([image_one,one_p, image_one,one_g1, image_one,one_g2], dim=0).to(device)\n",
    "\n",
    "    # seven!!!\n",
    "    true_p, true_g, latent = M(E(image_seven))\n",
    "    inter_p = inter\n",
    "    inter_g0 = torch.cat([inter, true_g[0, 1].unsqueeze(0).repeat(10,1)], dim=-1).to(device)\n",
    "    inter_g1 = torch.cat([true_g[0, 0].unsqueeze(0).repeat(10,1), inter], dim=-1).to(device)\n",
    "    label_seven = torch.tensor([0,1], dtype=torch.int64, device=device).unsqueeze(0)\n",
    "    seven_repeat = label_seven.repeat(10,1).to(device)\n",
    "    seven_latent_repeat = latent.repeat(10,1).to(device)\n",
    "    seven_p = G(seven_repeat, inter_p, true_g.repeat(10,1), seven_latent_repeat)\n",
    "    seven_g1 = G(seven_repeat, true_p.repeat(10,1), inter_g0, seven_latent_repeat)\n",
    "    seven_g2 = G(seven_repeat, true_p.repeat(10,1), inter_g1, seven_latent_repeat)\n",
    "    sample_seven = torch.cat([image_seven,seven_p,image_seven,seven_g1, image_seven,seven_g2], dim=0).to(device)\n",
    "    \n",
    "    sample = torch.cat([sample_one,sample_seven], dim=0).to(device)\n",
    "        \n",
    "    grid = torchvision.utils.make_grid(sample, nrow=11, normalize=True)\n",
    "    \n",
    "    save_image(grid, 'samples/{}_{}/grid_{}.png'.format(folder_path,args['description'],epoch))\n",
    "    \n",
    "    return sample, grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_loss = nn.CrossEntropyLoss().to(device)\n",
    "recon_loss = nn.MSELoss().to(device)\n",
    "adversarial_loss = nn.MSELoss().to(device)\n",
    "discrete_loss = nn.CrossEntropyLoss().to(device)\n",
    "code_loss = nn.MSELoss().to(device)\n",
    "# code_loss_P = nn.MSELoss().to(device)\n",
    "# pred_P_D_loss = nn.L1Loss().to(device)\n",
    "\n",
    "lambda_disc = 1\n",
    "lambda_code = 0.5\n",
    "args['description'] = 'sample_image_modified'\n",
    "from pretrain_ResNet import ResNet_3232\n",
    "pretrained_resnet = ResNet_3232(channels=1, num_classes=2).to(device)\n",
    "pretrained_resnet.load_state_dict(torch.load('pretrained_model/ResNet_3232_parameters_1_7.pt'))\n",
    "\n",
    "from Networks import Mapper, Predictor, Generator, Discriminator\n",
    "args['disc_dim'] = 2\n",
    "args['code_P_dim'] = 1\n",
    "args['code_G_dim'] = 2\n",
    "args['latent_dim'] = 32\n",
    "args['reduced_dim'] = args['code_P_dim'] + args['code_G_dim'] + args['latent_dim'] # 35\n",
    "\n",
    "E = nn.Sequential(*(list(pretrained_resnet.children())[:5])).to(device)\n",
    "M = Mapper(args).to(device)\n",
    "P = Predictor(args).to(device)\n",
    "G = Generator(args).to(device)\n",
    "D = Discriminator(args).to(device)\n",
    "\n",
    "\n",
    "M.apply(weights_init_normal)\n",
    "P.apply(weights_init_normal)\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "optimizer_P = torch.optim.Adam(itertools.chain(M.parameters(),P.parameters()), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(M.parameters(),G.parameters()), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "optimizer_D = torch.optim.Adam(itertools.chain(M.parameters(),D.parameters()), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "optimizer_info = torch.optim.Adam(itertools.chain(M.parameters(), P.parameters(),G.parameters(), D.parameters()), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "# optimizer_info_P = torch.optim.Adam(itertools.chain(M.parameters(), P.parameters(), G.parameters(), D.parameters()), \n",
    "#                                     lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if (device == 'cuda:{}'.format(args['GPU_NUM'])) else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if (device == 'cuda:{}'.format(args['GPU_NUM'])) else torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x, y in train_loader:\n",
    "#     x = x.to(device)\n",
    "#     y = y.to(device)\n",
    "#     break\n",
    "\n",
    "# plt.figure(figsize=(15,12))\n",
    "# for idx in range(20):\n",
    "#     plt.subplot(4,5,idx+1)\n",
    "#     plt.axis('off')\n",
    "#     plt.title('idx={}, label={}'.format(idx, y[idx].item()))\n",
    "#     plt.imshow(to_np(x[idx].permute(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts!\n",
      "[Epoch 0/200] [acc : 99.42] [P : 0.1049] [D : 0.0240] [G : 37.6082] [info: 0.3767] [code P: 0.0191] [code G: 0.0202] [time: 14.0]\n",
      "[Epoch 1/200] [acc : 99.73] [P : 0.0239] [D : 0.0318] [G : 13.7915] [info: 0.3535] [code P: 0.0140] [code G: 0.0185] [time: 28.3]\n",
      "[Epoch 2/200] [acc : 99.62] [P : 0.0121] [D : 0.0418] [G : 8.3311] [info: 0.3390] [code P: 0.0116] [code G: 0.0114] [time: 41.5]\n",
      "[Epoch 3/200] [acc : 99.62] [P : 0.0114] [D : 0.0238] [G : 9.3683] [info: 0.3554] [code P: 0.0185] [code G: 0.0172] [time: 55.2]\n",
      "[Epoch 4/200] [acc : 99.73] [P : 0.0145] [D : 0.0236] [G : 6.5987] [info: 0.3428] [code P: 0.0102] [code G: 0.0172] [time: 70.9]\n",
      "[Epoch 5/200] [acc : 99.58] [P : 0.0030] [D : 0.0168] [G : 6.5945] [info: 0.3306] [code P: 0.0101] [code G: 0.0066] [time: 87.4]\n",
      "[Epoch 6/200] [acc : 99.58] [P : 0.0035] [D : 0.1050] [G : 5.6449] [info: 0.3302] [code P: 0.0062] [code G: 0.0093] [time: 102.3]\n",
      "[Epoch 7/200] [acc : 99.50] [P : 0.1500] [D : 0.3495] [G : 8.8269] [info: 0.4015] [code P: 0.0233] [code G: 0.0259] [time: 117.2]\n",
      "[Epoch 8/200] [acc : 99.65] [P : 0.0040] [D : 0.2894] [G : 5.1163] [info: 0.3465] [code P: 0.0156] [code G: 0.0161] [time: 130.5]\n",
      "[Epoch 9/200] [acc : 99.69] [P : 0.0016] [D : 0.1621] [G : 5.7990] [info: 0.3428] [code P: 0.0114] [code G: 0.0155] [time: 144.0]\n",
      "[Epoch 10/200] [acc : 99.69] [P : 0.0012] [D : 0.2541] [G : 5.7323] [info: 0.3288] [code P: 0.0064] [code G: 0.0088] [time: 157.8]\n",
      "[Epoch 11/200] [acc : 99.65] [P : 0.0009] [D : 0.1073] [G : 4.9582] [info: 0.3408] [code P: 0.0096] [code G: 0.0109] [time: 171.4]\n",
      "[Epoch 12/200] [acc : 99.58] [P : 0.0008] [D : 0.1331] [G : 4.3147] [info: 0.3325] [code P: 0.0094] [code G: 0.0090] [time: 184.1]\n",
      "[Epoch 13/200] [acc : 99.58] [P : 0.0008] [D : 0.0681] [G : 4.9671] [info: 0.3265] [code P: 0.0058] [code G: 0.0072] [time: 196.5]\n",
      "[Epoch 14/200] [acc : 99.65] [P : 0.0005] [D : 0.1460] [G : 4.5400] [info: 0.3364] [code P: 0.0113] [code G: 0.0110] [time: 209.7]\n",
      "[Epoch 15/200] [acc : 99.65] [P : 0.0009] [D : 0.0262] [G : 5.6476] [info: 0.3342] [code P: 0.0130] [code G: 0.0078] [time: 225.2]\n",
      "[Epoch 16/200] [acc : 99.69] [P : 0.0005] [D : 0.1825] [G : 6.3521] [info: 0.3318] [code P: 0.0099] [code G: 0.0080] [time: 239.5]\n",
      "[Epoch 17/200] [acc : 99.62] [P : 0.0006] [D : 0.1110] [G : 4.7681] [info: 0.3287] [code P: 0.0068] [code G: 0.0085] [time: 251.8]\n",
      "[Epoch 18/200] [acc : 99.73] [P : 0.0006] [D : 0.1908] [G : 4.2827] [info: 0.3308] [code P: 0.0093] [code G: 0.0078] [time: 264.8]\n",
      "[Epoch 19/200] [acc : 99.65] [P : 0.0004] [D : 0.2092] [G : 3.7873] [info: 0.3270] [code P: 0.0077] [code G: 0.0059] [time: 277.3]\n",
      "[Epoch 20/200] [acc : 99.58] [P : 0.0011] [D : 0.1388] [G : 3.5230] [info: 0.3308] [code P: 0.0098] [code G: 0.0074] [time: 289.5]\n",
      "[Epoch 21/200] [acc : 99.58] [P : 0.0017] [D : 0.3288] [G : 3.8355] [info: 0.3375] [code P: 0.0086] [code G: 0.0091] [time: 303.0]\n",
      "[Epoch 22/200] [acc : 99.69] [P : 0.0007] [D : 0.1617] [G : 3.3397] [info: 0.3244] [code P: 0.0057] [code G: 0.0049] [time: 316.4]\n",
      "[Epoch 23/200] [acc : 99.69] [P : 0.0002] [D : 0.1292] [G : 4.5747] [info: 0.3319] [code P: 0.0106] [code G: 0.0074] [time: 329.5]\n",
      "[Epoch 24/200] [acc : 99.62] [P : 0.0068] [D : 0.2160] [G : 4.1619] [info: 0.3259] [code P: 0.0059] [code G: 0.0061] [time: 342.2]\n",
      "[Epoch 25/200] [acc : 99.69] [P : 0.0123] [D : 0.2746] [G : 4.6524] [info: 0.3675] [code P: 0.0174] [code G: 0.0173] [time: 354.4]\n",
      "[Epoch 26/200] [acc : 99.65] [P : 0.0093] [D : 0.2001] [G : 4.0809] [info: 0.3271] [code P: 0.0062] [code G: 0.0067] [time: 367.1]\n",
      "[Epoch 27/200] [acc : 99.58] [P : 0.1681] [D : 0.2158] [G : 6.9956] [info: 0.3302] [code P: 0.0066] [code G: 0.0057] [time: 379.7]\n",
      "[Epoch 28/200] [acc : 99.69] [P : 0.1861] [D : 0.2076] [G : 4.0023] [info: 0.3401] [code P: 0.0103] [code G: 0.0077] [time: 392.1]\n",
      "[Epoch 29/200] [acc : 99.65] [P : 0.0928] [D : 0.1314] [G : 4.2181] [info: 0.3220] [code P: 0.0038] [code G: 0.0049] [time: 405.2]\n",
      "[Epoch 30/200] [acc : 99.65] [P : 0.0009] [D : 0.1004] [G : 3.3978] [info: 0.3210] [code P: 0.0041] [code G: 0.0035] [time: 418.0]\n",
      "[Epoch 31/200] [acc : 99.65] [P : 0.0017] [D : 0.1075] [G : 3.6329] [info: 0.3224] [code P: 0.0036] [code G: 0.0052] [time: 430.4]\n",
      "[Epoch 32/200] [acc : 99.73] [P : 0.0006] [D : 0.1453] [G : 3.5349] [info: 0.3180] [code P: 0.0028] [code G: 0.0019] [time: 443.1]\n",
      "[Epoch 33/200] [acc : 99.69] [P : 0.0008] [D : 0.2430] [G : 3.1407] [info: 0.3229] [code P: 0.0051] [code G: 0.0044] [time: 454.7]\n",
      "[Epoch 34/200] [acc : 99.73] [P : 0.0003] [D : 0.2389] [G : 2.7456] [info: 0.3237] [code P: 0.0066] [code G: 0.0038] [time: 467.3]\n",
      "[Epoch 35/200] [acc : 99.62] [P : 0.0119] [D : 0.2375] [G : 3.6938] [info: 0.3271] [code P: 0.0063] [code G: 0.0063] [time: 480.2]\n",
      "[Epoch 36/200] [acc : 99.65] [P : 0.0011] [D : 0.2965] [G : 3.4597] [info: 0.3205] [code P: 0.0040] [code G: 0.0030] [time: 492.3]\n",
      "[Epoch 37/200] [acc : 99.65] [P : 0.0005] [D : 0.1206] [G : 3.4438] [info: 0.3225] [code P: 0.0053] [code G: 0.0038] [time: 505.0]\n",
      "[Epoch 38/200] [acc : 99.62] [P : 0.0003] [D : 0.1792] [G : 3.9389] [info: 0.3295] [code P: 0.0098] [code G: 0.0061] [time: 518.5]\n",
      "[Epoch 39/200] [acc : 99.73] [P : 0.0016] [D : 0.1664] [G : 3.1209] [info: 0.3237] [code P: 0.0053] [code G: 0.0048] [time: 532.7]\n",
      "[Epoch 40/200] [acc : 99.62] [P : 0.0008] [D : 0.2021] [G : 3.4971] [info: 0.3196] [code P: 0.0027] [code G: 0.0036] [time: 547.5]\n",
      "[Epoch 41/200] [acc : 99.58] [P : 0.0266] [D : 0.2357] [G : 4.3468] [info: 0.3220] [code P: 0.0048] [code G: 0.0039] [time: 562.3]\n",
      "[Epoch 42/200] [acc : 99.65] [P : 0.0005] [D : 0.1203] [G : 3.0235] [info: 0.3188] [code P: 0.0032] [code G: 0.0023] [time: 576.5]\n",
      "[Epoch 43/200] [acc : 99.62] [P : 0.0074] [D : 0.2758] [G : 3.3312] [info: 0.3176] [code P: 0.0024] [code G: 0.0019] [time: 589.8]\n",
      "[Epoch 44/200] [acc : 99.73] [P : 0.0003] [D : 0.2271] [G : 3.7779] [info: 0.3185] [code P: 0.0026] [code G: 0.0026] [time: 603.6]\n",
      "[Epoch 45/200] [acc : 99.65] [P : 0.0001] [D : 0.2436] [G : 4.2178] [info: 0.3190] [code P: 0.0029] [code G: 0.0028] [time: 616.7]\n",
      "[Epoch 46/200] [acc : 99.69] [P : 0.0694] [D : 0.1714] [G : 5.5737] [info: 0.3196] [code P: 0.0034] [code G: 0.0029] [time: 629.5]\n",
      "[Epoch 47/200] [acc : 99.65] [P : 0.0003] [D : 0.2662] [G : 3.0893] [info: 0.3171] [code P: 0.0020] [code G: 0.0017] [time: 643.3]\n",
      "[Epoch 48/200] [acc : 99.58] [P : 0.0001] [D : 0.2164] [G : 3.6158] [info: 0.3177] [code P: 0.0025] [code G: 0.0019] [time: 657.6]\n",
      "[Epoch 49/200] [acc : 99.65] [P : 0.0001] [D : 0.1888] [G : 3.5327] [info: 0.3192] [code P: 0.0025] [code G: 0.0034] [time: 670.8]\n",
      "[Epoch 50/200] [acc : 99.65] [P : 0.0045] [D : 0.2239] [G : 3.5153] [info: 0.3177] [code P: 0.0019] [code G: 0.0023] [time: 682.8]\n",
      "[Epoch 51/200] [acc : 99.73] [P : 0.0003] [D : 0.2030] [G : 3.6601] [info: 0.3178] [code P: 0.0027] [code G: 0.0018] [time: 696.0]\n",
      "[Epoch 52/200] [acc : 99.73] [P : 0.0006] [D : 0.2928] [G : 3.1106] [info: 0.3158] [code P: 0.0014] [code G: 0.0011] [time: 710.7]\n",
      "[Epoch 53/200] [acc : 99.69] [P : 0.0003] [D : 0.2734] [G : 2.9331] [info: 0.3171] [code P: 0.0022] [code G: 0.0016] [time: 725.2]\n",
      "[Epoch 54/200] [acc : 99.62] [P : 0.0002] [D : 0.2351] [G : 3.5186] [info: 0.3182] [code P: 0.0029] [code G: 0.0019] [time: 740.4]\n",
      "[Epoch 55/200] [acc : 99.65] [P : 0.0041] [D : 0.3350] [G : 3.9258] [info: 0.3169] [code P: 0.0023] [code G: 0.0013] [time: 754.5]\n",
      "[Epoch 56/200] [acc : 99.65] [P : 0.0552] [D : 0.1981] [G : 5.6707] [info: 0.3205] [code P: 0.0041] [code G: 0.0031] [time: 768.1]\n",
      "[Epoch 57/200] [acc : 99.77] [P : 0.0003] [D : 0.0742] [G : 3.0430] [info: 0.3174] [code P: 0.0024] [code G: 0.0015] [time: 782.7]\n",
      "[Epoch 58/200] [acc : 99.73] [P : 0.0005] [D : 0.2612] [G : 2.8579] [info: 0.3163] [code P: 0.0014] [code G: 0.0016] [time: 795.2]\n",
      "[Epoch 59/200] [acc : 99.73] [P : 0.0017] [D : 0.1497] [G : 2.8184] [info: 0.3185] [code P: 0.0027] [code G: 0.0018] [time: 807.8]\n",
      "[Epoch 60/200] [acc : 99.69] [P : 0.0008] [D : 0.2329] [G : 3.7061] [info: 0.3163] [code P: 0.0018] [code G: 0.0012] [time: 820.9]\n",
      "[Epoch 61/200] [acc : 99.73] [P : 0.0007] [D : 0.0283] [G : 4.0579] [info: 0.3175] [code P: 0.0026] [code G: 0.0016] [time: 834.8]\n",
      "[Epoch 62/200] [acc : 99.50] [P : 0.0007] [D : 0.2316] [G : 3.2558] [info: 0.3180] [code P: 0.0027] [code G: 0.0017] [time: 848.4]\n",
      "[Epoch 63/200] [acc : 99.77] [P : 0.0013] [D : 0.1064] [G : 3.1589] [info: 0.3161] [code P: 0.0015] [code G: 0.0012] [time: 861.7]\n",
      "[Epoch 64/200] [acc : 99.69] [P : 0.0003] [D : 0.1260] [G : 3.1770] [info: 0.3154] [code P: 0.0012] [code G: 0.0009] [time: 875.0]\n",
      "[Epoch 65/200] [acc : 99.69] [P : 0.2932] [D : 0.2890] [G : 4.6673] [info: 0.3438] [code P: 0.0025] [code G: 0.0023] [time: 889.1]\n",
      "[Epoch 66/200] [acc : 99.54] [P : 0.0013] [D : 0.2433] [G : 3.3446] [info: 0.3154] [code P: 0.0010] [code G: 0.0011] [time: 901.9]\n",
      "[Epoch 67/200] [acc : 99.69] [P : 0.0028] [D : 0.1418] [G : 3.0906] [info: 0.3150] [code P: 0.0009] [code G: 0.0009] [time: 915.2]\n",
      "[Epoch 68/200] [acc : 99.58] [P : 0.1100] [D : 0.2232] [G : 4.2731] [info: 0.3222] [code P: 0.0044] [code G: 0.0021] [time: 928.2]\n",
      "[Epoch 69/200] [acc : 99.69] [P : 0.0005] [D : 0.2261] [G : 2.9177] [info: 0.3172] [code P: 0.0023] [code G: 0.0016] [time: 940.8]\n",
      "[Epoch 70/200] [acc : 99.73] [P : 0.0002] [D : 0.2500] [G : 2.8107] [info: 0.3154] [code P: 0.0014] [code G: 0.0007] [time: 954.5]\n",
      "[Epoch 71/200] [acc : 99.69] [P : 0.0107] [D : 0.3126] [G : 2.6540] [info: 0.3180] [code P: 0.0029] [code G: 0.0015] [time: 967.9]\n",
      "[Epoch 72/200] [acc : 99.73] [P : 0.0012] [D : 0.0944] [G : 3.0025] [info: 0.3151] [code P: 0.0010] [code G: 0.0007] [time: 980.9]\n",
      "[Epoch 73/200] [acc : 99.69] [P : 0.0254] [D : 0.1395] [G : 3.6911] [info: 0.3170] [code P: 0.0022] [code G: 0.0015] [time: 994.7]\n",
      "[Epoch 74/200] [acc : 99.35] [P : 0.3291] [D : 0.2384] [G : 5.6460] [info: 0.3199] [code P: 0.0035] [code G: 0.0015] [time: 1007.6]\n",
      "[Epoch 75/200] [acc : 99.73] [P : 0.0001] [D : 0.1695] [G : 2.4950] [info: 0.3167] [code P: 0.0012] [code G: 0.0008] [time: 1021.6]\n",
      "[Epoch 76/200] [acc : 99.69] [P : 0.0005] [D : 0.2502] [G : 2.8979] [info: 0.3147] [code P: 0.0009] [code G: 0.0006] [time: 1035.7]\n",
      "[Epoch 77/200] [acc : 99.62] [P : 0.0025] [D : 0.2105] [G : 3.2054] [info: 0.3154] [code P: 0.0013] [code G: 0.0008] [time: 1049.4]\n",
      "[Epoch 78/200] [acc : 99.58] [P : 0.0019] [D : 0.2978] [G : 3.2474] [info: 0.3158] [code P: 0.0014] [code G: 0.0006] [time: 1062.3]\n",
      "[Epoch 79/200] [acc : 99.69] [P : 0.0021] [D : 0.1666] [G : 3.8024] [info: 0.3146] [code P: 0.0007] [code G: 0.0006] [time: 1075.2]\n",
      "[Epoch 80/200] [acc : 99.69] [P : 0.0011] [D : 0.1167] [G : 4.4685] [info: 0.3150] [code P: 0.0010] [code G: 0.0006] [time: 1088.1]\n",
      "[Epoch 81/200] [acc : 99.73] [P : 0.0050] [D : 0.3606] [G : 3.7274] [info: 0.3174] [code P: 0.0026] [code G: 0.0015] [time: 1102.1]\n",
      "[Epoch 82/200] [acc : 99.69] [P : 0.0053] [D : 0.1487] [G : 3.2626] [info: 0.3146] [code P: 0.0009] [code G: 0.0005] [time: 1114.7]\n",
      "[Epoch 83/200] [acc : 99.65] [P : 0.0005] [D : 0.1205] [G : 2.9920] [info: 0.3159] [code P: 0.0012] [code G: 0.0005] [time: 1128.2]\n",
      "[Epoch 84/200] [acc : 99.62] [P : 0.0016] [D : 0.1774] [G : 3.3078] [info: 0.3147] [code P: 0.0008] [code G: 0.0007] [time: 1141.0]\n",
      "[Epoch 85/200] [acc : 99.62] [P : 0.0522] [D : 0.1821] [G : 5.0705] [info: 0.3153] [code P: 0.0011] [code G: 0.0009] [time: 1154.3]\n",
      "[Epoch 86/200] [acc : 99.69] [P : 0.0003] [D : 0.1300] [G : 2.8138] [info: 0.3142] [code P: 0.0006] [code G: 0.0003] [time: 1167.6]\n",
      "[Epoch 87/200] [acc : 99.58] [P : 0.0001] [D : 0.1639] [G : 3.3548] [info: 0.3150] [code P: 0.0011] [code G: 0.0007] [time: 1181.3]\n",
      "[Epoch 88/200] [acc : 99.69] [P : 0.0005] [D : 0.2657] [G : 3.0895] [info: 0.3150] [code P: 0.0010] [code G: 0.0008] [time: 1193.7]\n",
      "[Epoch 89/200] [acc : 99.62] [P : 0.0002] [D : 0.1300] [G : 3.1631] [info: 0.3142] [code P: 0.0006] [code G: 0.0004] [time: 1206.8]\n",
      "[Epoch 90/200] [acc : 99.69] [P : 0.0004] [D : 0.2100] [G : 3.8922] [info: 0.3147] [code P: 0.0010] [code G: 0.0004] [time: 1219.4]\n",
      "[Epoch 91/200] [acc : 99.73] [P : 0.0002] [D : 0.1123] [G : 2.9267] [info: 0.3143] [code P: 0.0007] [code G: 0.0003] [time: 1231.6]\n",
      "[Epoch 92/200] [acc : 99.62] [P : 0.0219] [D : 0.1796] [G : 3.7886] [info: 0.3174] [code P: 0.0028] [code G: 0.0013] [time: 1244.7]\n",
      "[Epoch 93/200] [acc : 99.62] [P : 0.0002] [D : 0.1380] [G : 2.4654] [info: 0.3142] [code P: 0.0006] [code G: 0.0003] [time: 1257.4]\n",
      "[Epoch 94/200] [acc : 99.65] [P : 0.0005] [D : 0.1552] [G : 3.0821] [info: 0.3147] [code P: 0.0011] [code G: 0.0003] [time: 1271.0]\n",
      "[Epoch 95/200] [acc : 99.73] [P : 0.0781] [D : 0.2510] [G : 4.1557] [info: 0.3150] [code P: 0.0011] [code G: 0.0007] [time: 1284.0]\n",
      "[Epoch 96/200] [acc : 99.65] [P : 0.0001] [D : 0.1923] [G : 3.1028] [info: 0.3142] [code P: 0.0006] [code G: 0.0003] [time: 1296.5]\n",
      "[Epoch 97/200] [acc : 99.65] [P : 0.0007] [D : 0.1326] [G : 4.7579] [info: 0.3152] [code P: 0.0015] [code G: 0.0004] [time: 1309.1]\n",
      "[Epoch 98/200] [acc : 99.69] [P : 0.0006] [D : 0.1185] [G : 2.5777] [info: 0.3139] [code P: 0.0005] [code G: 0.0002] [time: 1321.0]\n",
      "[Epoch 99/200] [acc : 99.65] [P : 0.0079] [D : 0.2426] [G : 3.7107] [info: 0.3152] [code P: 0.0013] [code G: 0.0004] [time: 1333.1]\n",
      "[Epoch 100/200] [acc : 99.65] [P : 0.0015] [D : 0.4306] [G : 2.6738] [info: 0.3139] [code P: 0.0004] [code G: 0.0003] [time: 1345.3]\n",
      "[Epoch 101/200] [acc : 99.69] [P : 0.0032] [D : 0.1598] [G : 3.6515] [info: 0.3141] [code P: 0.0004] [code G: 0.0003] [time: 1358.5]\n",
      "[Epoch 102/200] [acc : 99.62] [P : 0.0012] [D : 0.1033] [G : 2.9878] [info: 0.3141] [code P: 0.0006] [code G: 0.0002] [time: 1374.7]\n",
      "[Epoch 103/200] [acc : 99.69] [P : 0.0044] [D : 0.1452] [G : 2.6263] [info: 0.3141] [code P: 0.0006] [code G: 0.0002] [time: 1389.4]\n",
      "[Epoch 104/200] [acc : 99.69] [P : 0.0008] [D : 0.2536] [G : 2.4358] [info: 0.3139] [code P: 0.0005] [code G: 0.0001] [time: 1402.6]\n",
      "[Epoch 105/200] [acc : 99.65] [P : 0.0053] [D : 0.2191] [G : 4.2018] [info: 0.3138] [code P: 0.0004] [code G: 0.0001] [time: 1415.8]\n",
      "[Epoch 106/200] [acc : 99.62] [P : 0.0003] [D : 0.2704] [G : 3.7225] [info: 0.3146] [code P: 0.0010] [code G: 0.0003] [time: 1428.3]\n",
      "[Epoch 107/200] [acc : 99.73] [P : 0.0000] [D : 0.3855] [G : 4.0041] [info: 0.3189] [code P: 0.0008] [code G: 0.0002] [time: 1441.4]\n",
      "[Epoch 108/200] [acc : 99.69] [P : 0.0008] [D : 0.1134] [G : 3.2745] [info: 0.3138] [code P: 0.0004] [code G: 0.0002] [time: 1454.3]\n",
      "[Epoch 109/200] [acc : 99.77] [P : 0.0004] [D : 0.1649] [G : 3.4615] [info: 0.3137] [code P: 0.0003] [code G: 0.0001] [time: 1466.6]\n",
      "[Epoch 110/200] [acc : 99.65] [P : 0.0014] [D : 0.0603] [G : 3.2980] [info: 0.3138] [code P: 0.0004] [code G: 0.0002] [time: 1478.8]\n",
      "[Epoch 111/200] [acc : 99.62] [P : 0.0021] [D : 0.1003] [G : 3.1809] [info: 0.3139] [code P: 0.0006] [code G: 0.0001] [time: 1491.5]\n",
      "[Epoch 112/200] [acc : 99.73] [P : 0.0017] [D : 0.1887] [G : 3.2055] [info: 0.3138] [code P: 0.0004] [code G: 0.0002] [time: 1504.0]\n",
      "[Epoch 113/200] [acc : 99.69] [P : 0.0010] [D : 0.2565] [G : 3.0508] [info: 0.3140] [code P: 0.0006] [code G: 0.0001] [time: 1516.3]\n",
      "[Epoch 114/200] [acc : 99.73] [P : 0.0007] [D : 0.2631] [G : 3.3760] [info: 0.3144] [code P: 0.0010] [code G: 0.0001] [time: 1528.1]\n",
      "[Epoch 115/200] [acc : 99.73] [P : 0.0005] [D : 0.2585] [G : 2.6754] [info: 0.3136] [code P: 0.0002] [code G: 0.0001] [time: 1540.9]\n",
      "[Epoch 116/200] [acc : 99.58] [P : 0.0040] [D : 0.2646] [G : 2.9270] [info: 0.3135] [code P: 0.0002] [code G: 0.0001] [time: 1553.3]\n",
      "[Epoch 117/200] [acc : 99.73] [P : 0.0002] [D : 0.2360] [G : 3.9494] [info: 0.3141] [code P: 0.0008] [code G: 0.0001] [time: 1565.2]\n",
      "[Epoch 118/200] [acc : 99.65] [P : 0.0422] [D : 0.4021] [G : 2.9167] [info: 0.3139] [code P: 0.0005] [code G: 0.0001] [time: 1577.5]\n",
      "[Epoch 119/200] [acc : 99.73] [P : 0.0129] [D : 0.1509] [G : 4.1316] [info: 0.3139] [code P: 0.0003] [code G: 0.0001] [time: 1590.5]\n",
      "[Epoch 120/200] [acc : 99.58] [P : 0.0006] [D : 0.0623] [G : 5.1006] [info: 0.3141] [code P: 0.0007] [code G: 0.0001] [time: 1603.2]\n",
      "[Epoch 121/200] [acc : 99.73] [P : 0.0003] [D : 0.2416] [G : 2.6471] [info: 0.3136] [code P: 0.0003] [code G: 0.0000] [time: 1615.4]\n",
      "[Epoch 122/200] [acc : 99.65] [P : 0.0010] [D : 0.3241] [G : 3.3218] [info: 0.3136] [code P: 0.0003] [code G: 0.0000] [time: 1628.3]\n",
      "[Epoch 123/200] [acc : 99.65] [P : 0.0000] [D : 0.2375] [G : 3.2848] [info: 0.3140] [code P: 0.0006] [code G: 0.0000] [time: 1641.2]\n",
      "[Epoch 124/200] [acc : 99.58] [P : 0.0001] [D : 0.2879] [G : 2.8600] [info: 0.3138] [code P: 0.0004] [code G: 0.0000] [time: 1654.7]\n",
      "[Epoch 125/200] [acc : 99.62] [P : 0.0012] [D : 0.3859] [G : 3.2966] [info: 0.3137] [code P: 0.0004] [code G: 0.0000] [time: 1668.2]\n",
      "[Epoch 126/200] [acc : 99.65] [P : 0.0003] [D : 0.0988] [G : 2.3456] [info: 0.3136] [code P: 0.0003] [code G: 0.0001] [time: 1681.2]\n",
      "[Epoch 127/200] [acc : 99.65] [P : 0.0017] [D : 0.1682] [G : 3.6653] [info: 0.3135] [code P: 0.0002] [code G: 0.0000] [time: 1695.0]\n",
      "[Epoch 128/200] [acc : 99.58] [P : 0.0001] [D : 0.1833] [G : 2.8865] [info: 0.3141] [code P: 0.0004] [code G: 0.0001] [time: 1707.5]\n",
      "[Epoch 129/200] [acc : 99.69] [P : 0.0047] [D : 0.3650] [G : 3.7679] [info: 0.3139] [code P: 0.0006] [code G: 0.0001] [time: 1720.1]\n",
      "[Epoch 130/200] [acc : 99.62] [P : 0.0010] [D : 0.3685] [G : 3.7493] [info: 0.3138] [code P: 0.0004] [code G: 0.0001] [time: 1732.4]\n",
      "[Epoch 131/200] [acc : 99.69] [P : 0.0007] [D : 0.1407] [G : 3.2259] [info: 0.3136] [code P: 0.0003] [code G: 0.0001] [time: 1745.5]\n",
      "[Epoch 132/200] [acc : 99.69] [P : 0.0002] [D : 0.1058] [G : 3.3557] [info: 0.3137] [code P: 0.0004] [code G: 0.0000] [time: 1758.3]\n",
      "[Epoch 133/200] [acc : 99.73] [P : 0.0002] [D : 0.4563] [G : 2.7649] [info: 0.3135] [code P: 0.0002] [code G: 0.0000] [time: 1771.1]\n",
      "[Epoch 134/200] [acc : 99.69] [P : 0.0003] [D : 0.2155] [G : 3.0165] [info: 0.3135] [code P: 0.0002] [code G: 0.0001] [time: 1783.5]\n",
      "[Epoch 135/200] [acc : 99.65] [P : 0.0002] [D : 0.1124] [G : 3.0777] [info: 0.3136] [code P: 0.0003] [code G: 0.0001] [time: 1796.9]\n",
      "[Epoch 136/200] [acc : 99.69] [P : 0.0002] [D : 0.2657] [G : 3.9718] [info: 0.3136] [code P: 0.0003] [code G: 0.0000] [time: 1809.6]\n",
      "[Epoch 137/200] [acc : 99.58] [P : 0.0001] [D : 0.1801] [G : 2.8593] [info: 0.3135] [code P: 0.0002] [code G: 0.0000] [time: 1821.8]\n",
      "[Epoch 138/200] [acc : 99.65] [P : 0.0025] [D : 0.3613] [G : 3.0062] [info: 0.3135] [code P: 0.0002] [code G: 0.0001] [time: 1834.1]\n",
      "[Epoch 139/200] [acc : 99.62] [P : 0.0005] [D : 0.3757] [G : 3.1745] [info: 0.3136] [code P: 0.0002] [code G: 0.0001] [time: 1846.6]\n",
      "[Epoch 140/200] [acc : 99.65] [P : 0.0013] [D : 0.1692] [G : 2.9993] [info: 0.3138] [code P: 0.0005] [code G: 0.0000] [time: 1859.2]\n",
      "[Epoch 141/200] [acc : 99.65] [P : 0.0006] [D : 0.2005] [G : 3.5664] [info: 0.3139] [code P: 0.0005] [code G: 0.0002] [time: 1871.8]\n",
      "[Epoch 142/200] [acc : 99.65] [P : 0.0015] [D : 0.2312] [G : 2.8948] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 1884.7]\n",
      "[Epoch 143/200] [acc : 99.65] [P : 0.0174] [D : 0.2086] [G : 3.5918] [info: 0.3147] [code P: 0.0003] [code G: 0.0000] [time: 1897.0]\n",
      "[Epoch 144/200] [acc : 99.69] [P : 0.0100] [D : 0.1774] [G : 2.6247] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 1909.8]\n",
      "[Epoch 145/200] [acc : 99.62] [P : 0.0207] [D : 0.1786] [G : 3.7048] [info: 0.3135] [code P: 0.0002] [code G: 0.0000] [time: 1922.7]\n",
      "[Epoch 146/200] [acc : 99.73] [P : 0.0005] [D : 0.2281] [G : 2.9993] [info: 0.3151] [code P: 0.0002] [code G: 0.0000] [time: 1934.9]\n",
      "[Epoch 147/200] [acc : 99.62] [P : 0.0131] [D : 0.3334] [G : 3.5588] [info: 0.3135] [code P: 0.0002] [code G: 0.0000] [time: 1947.5]\n",
      "[Epoch 148/200] [acc : 99.65] [P : 0.0006] [D : 0.1500] [G : 3.3534] [info: 0.3135] [code P: 0.0002] [code G: 0.0000] [time: 1960.9]\n",
      "[Epoch 149/200] [acc : 99.69] [P : 0.0002] [D : 0.3323] [G : 3.5942] [info: 0.3135] [code P: 0.0002] [code G: 0.0000] [time: 1973.8]\n",
      "[Epoch 150/200] [acc : 99.65] [P : 0.0033] [D : 0.1059] [G : 3.4868] [info: 0.3138] [code P: 0.0004] [code G: 0.0001] [time: 1986.4]\n",
      "[Epoch 151/200] [acc : 99.65] [P : 0.0007] [D : 0.5857] [G : 3.8172] [info: 0.3137] [code P: 0.0004] [code G: 0.0000] [time: 1999.3]\n",
      "[Epoch 152/200] [acc : 99.62] [P : 0.0003] [D : 0.1929] [G : 3.6973] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2012.1]\n",
      "[Epoch 153/200] [acc : 99.65] [P : 0.0004] [D : 0.2691] [G : 3.1172] [info: 0.3135] [code P: 0.0002] [code G: 0.0000] [time: 2024.7]\n",
      "[Epoch 154/200] [acc : 99.65] [P : 0.0434] [D : 0.1591] [G : 6.5632] [info: 0.3135] [code P: 0.0002] [code G: 0.0000] [time: 2038.4]\n",
      "[Epoch 155/200] [acc : 99.65] [P : 0.0002] [D : 0.0938] [G : 3.3242] [info: 0.3136] [code P: 0.0003] [code G: 0.0000] [time: 2051.3]\n",
      "[Epoch 156/200] [acc : 99.65] [P : 0.0011] [D : 0.1700] [G : 2.6442] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2064.6]\n",
      "[Epoch 157/200] [acc : 99.69] [P : 0.0064] [D : 0.1231] [G : 2.8236] [info: 0.3136] [code P: 0.0003] [code G: 0.0000] [time: 2077.5]\n",
      "[Epoch 158/200] [acc : 99.69] [P : 0.0010] [D : 0.1262] [G : 2.8922] [info: 0.3154] [code P: 0.0001] [code G: 0.0000] [time: 2090.4]\n",
      "[Epoch 159/200] [acc : 99.50] [P : 0.0004] [D : 0.1128] [G : 4.3505] [info: 0.3137] [code P: 0.0004] [code G: 0.0001] [time: 2102.6]\n",
      "[Epoch 160/200] [acc : 99.69] [P : 0.0001] [D : 0.2105] [G : 3.5191] [info: 0.3134] [code P: 0.0002] [code G: 0.0000] [time: 2115.3]\n",
      "[Epoch 161/200] [acc : 99.69] [P : 0.0014] [D : 0.1696] [G : 2.7005] [info: 0.3135] [code P: 0.0002] [code G: 0.0000] [time: 2127.7]\n",
      "[Epoch 162/200] [acc : 99.65] [P : 0.0113] [D : 0.2469] [G : 3.4321] [info: 0.3148] [code P: 0.0007] [code G: 0.0001] [time: 2141.2]\n",
      "[Epoch 163/200] [acc : 99.62] [P : 0.0017] [D : 0.1930] [G : 2.5735] [info: 0.3134] [code P: 0.0001] [code G: 0.0001] [time: 2154.0]\n",
      "[Epoch 164/200] [acc : 99.62] [P : 0.0012] [D : 0.0834] [G : 4.1746] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2167.3]\n",
      "[Epoch 165/200] [acc : 99.69] [P : 0.0002] [D : 0.1719] [G : 3.1353] [info: 0.3135] [code P: 0.0002] [code G: 0.0000] [time: 2180.8]\n",
      "[Epoch 166/200] [acc : 99.73] [P : 0.0002] [D : 0.0982] [G : 3.0553] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2194.4]\n",
      "[Epoch 167/200] [acc : 99.65] [P : 0.0001] [D : 0.2611] [G : 3.0895] [info: 0.3133] [code P: 0.0001] [code G: 0.0000] [time: 2208.1]\n",
      "[Epoch 168/200] [acc : 99.65] [P : 0.0001] [D : 0.2161] [G : 2.9400] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2222.5]\n",
      "[Epoch 169/200] [acc : 99.65] [P : 0.0016] [D : 0.4497] [G : 2.8227] [info: 0.3135] [code P: 0.0002] [code G: 0.0000] [time: 2235.3]\n",
      "[Epoch 170/200] [acc : 99.65] [P : 0.0024] [D : 0.1392] [G : 2.5145] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2251.0]\n",
      "[Epoch 171/200] [acc : 99.69] [P : 0.0003] [D : 0.4356] [G : 3.0988] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2266.0]\n",
      "[Epoch 172/200] [acc : 99.58] [P : 0.0009] [D : 0.2798] [G : 2.7275] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2280.5]\n",
      "[Epoch 173/200] [acc : 99.62] [P : 0.0017] [D : 0.2727] [G : 2.9488] [info: 0.3136] [code P: 0.0003] [code G: 0.0001] [time: 2295.7]\n",
      "[Epoch 174/200] [acc : 99.69] [P : 0.0005] [D : 0.1385] [G : 2.7351] [info: 0.3136] [code P: 0.0001] [code G: 0.0002] [time: 2313.5]\n",
      "[Epoch 175/200] [acc : 99.62] [P : 0.0005] [D : 0.2008] [G : 2.5634] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2330.4]\n",
      "[Epoch 176/200] [acc : 99.58] [P : 0.0004] [D : 0.1033] [G : 2.8368] [info: 0.3133] [code P: 0.0001] [code G: 0.0000] [time: 2348.1]\n",
      "[Epoch 177/200] [acc : 99.73] [P : 0.0002] [D : 0.1179] [G : 2.8588] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2364.9]\n",
      "[Epoch 178/200] [acc : 99.69] [P : 0.0005] [D : 0.2857] [G : 2.8967] [info: 0.3136] [code P: 0.0002] [code G: 0.0001] [time: 2380.7]\n",
      "[Epoch 179/200] [acc : 99.69] [P : 0.0016] [D : 0.2530] [G : 2.9024] [info: 0.3133] [code P: 0.0001] [code G: 0.0000] [time: 2396.1]\n",
      "[Epoch 180/200] [acc : 99.65] [P : 0.0081] [D : 0.1968] [G : 2.6213] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2411.9]\n",
      "[Epoch 181/200] [acc : 99.62] [P : 0.0004] [D : 0.2209] [G : 3.1996] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2428.1]\n",
      "[Epoch 182/200] [acc : 99.65] [P : 0.0008] [D : 0.0798] [G : 2.3420] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2444.3]\n",
      "[Epoch 183/200] [acc : 99.62] [P : 0.0006] [D : 0.2726] [G : 2.7290] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2459.3]\n",
      "[Epoch 184/200] [acc : 99.58] [P : 0.0030] [D : 0.2430] [G : 4.3224] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2472.7]\n",
      "[Epoch 185/200] [acc : 99.73] [P : 0.0007] [D : 0.1549] [G : 3.0036] [info: 0.3136] [code P: 0.0003] [code G: 0.0001] [time: 2488.2]\n",
      "[Epoch 186/200] [acc : 99.62] [P : 0.0003] [D : 0.2991] [G : 3.0700] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2504.2]\n",
      "[Epoch 187/200] [acc : 99.65] [P : 0.0003] [D : 0.4785] [G : 3.1468] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2518.7]\n",
      "[Epoch 188/200] [acc : 99.65] [P : 0.0007] [D : 0.2440] [G : 2.7915] [info: 0.3137] [code P: 0.0003] [code G: 0.0001] [time: 2532.6]\n",
      "[Epoch 189/200] [acc : 99.69] [P : 0.0006] [D : 0.3474] [G : 3.2521] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2546.9]\n",
      "[Epoch 190/200] [acc : 99.65] [P : 0.1358] [D : 0.2885] [G : 3.2725] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2560.4]\n",
      "[Epoch 191/200] [acc : 99.58] [P : 0.0007] [D : 0.1697] [G : 4.3060] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2576.3]\n",
      "[Epoch 192/200] [acc : 99.69] [P : 0.0014] [D : 0.2710] [G : 3.2314] [info: 0.3133] [code P: 0.0001] [code G: 0.0000] [time: 2593.1]\n",
      "[Epoch 193/200] [acc : 99.62] [P : 0.1009] [D : 0.2516] [G : 4.8090] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2608.4]\n",
      "[Epoch 194/200] [acc : 99.65] [P : 0.0004] [D : 0.0484] [G : 2.9990] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2625.9]\n",
      "[Epoch 195/200] [acc : 99.65] [P : 0.0023] [D : 0.1417] [G : 2.8183] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2642.6]\n",
      "[Epoch 196/200] [acc : 99.69] [P : 0.0002] [D : 0.2055] [G : 2.6596] [info: 0.3136] [code P: 0.0003] [code G: 0.0000] [time: 2658.0]\n",
      "[Epoch 197/200] [acc : 99.73] [P : 0.0050] [D : 0.1576] [G : 2.9324] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2673.2]\n",
      "[Epoch 198/200] [acc : 99.73] [P : 0.0011] [D : 0.3676] [G : 2.5022] [info: 0.3134] [code P: 0.0001] [code G: 0.0000] [time: 2688.5]\n",
      "[Epoch 199/200] [acc : 99.69] [P : 0.0003] [D : 0.2238] [G : 3.0593] [info: 0.3133] [code P: 0.0001] [code G: 0.0000] [time: 2703.2]\n"
     ]
    }
   ],
   "source": [
    "image_one = x[13].unsqueeze(0).clone()\n",
    "image_seven = x[0].unsqueeze(0).clone()\n",
    "\n",
    "import torchvision\n",
    "from torch.utils import tensorboard\n",
    "today = datetime.date.today()\n",
    "loss_writer = tensorboard.SummaryWriter('logs/MNIST/loss/{}_{}'.format(today,args['description']))\n",
    "image_writer = tensorboard.SummaryWriter('logs/MNIST/image/{}_{}'.format(today,args['description']))\n",
    "\n",
    "start = time.time() ; print('Training starts!')\n",
    "\n",
    "for epoch in range(args['Epochs']):\n",
    "    M.train() ; P.train() ; G.train() ; D.train()\n",
    "    for i, (imgs,labels) in enumerate(train_loader):\n",
    "        batch_size = imgs.shape[0]\n",
    "        real = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False).to(device)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False).to(device)\n",
    "        \n",
    "        real_imgs = Variable(imgs.type(FloatTensor)).to(device)\n",
    "        real_labels = to_discrete(labels.numpy(), args['n_classes']).to(device)\n",
    "        labels = labels.to(device)\n",
    "        encoded = E(real_imgs)        \n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Predictor\n",
    "        # -----------------\n",
    "        optimizer_P.zero_grad()\n",
    "        code_P, code_G, latent = M(encoded.clone())\n",
    "        predicted= P(code_P)\n",
    "        loss_P = predict_loss(predicted, real_labels)\n",
    "\n",
    "        loss_P.backward(retain_graph=True)\n",
    "        optimizer_P.step()\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "        code_P, code_G, latent = M(encoded.clone())\n",
    "        fake_imgs = G(real_labels, code_P, code_G, latent)\n",
    "        reality, _, _, _ = D(fake_imgs)\n",
    "        \n",
    "        loss_adv = adversarial_loss(reality, real) # fake_imgs의 분류(D) 결과가 최대한 1(real)로 분류되도록 G 학습\n",
    "        loss_recon = recon_loss(real_imgs, fake_imgs)\n",
    "        # alpha = 0.1\n",
    "        # loss_G = (alpha*loss_adv + (1-alpha)*loss_recon)\n",
    "        loss_G = (loss_adv + 100*loss_recon)\n",
    "        loss_G.backward(retain_graph=True)\n",
    "        optimizer_G.step()\n",
    "    \n",
    "        # -----------------\n",
    "        #  Train Discriminator\n",
    "        # -----------------\n",
    "        optimizer_D.zero_grad()\n",
    "        code_P, code_G, latent = M(encoded.clone())\n",
    "        # real or fake pred score\n",
    "        pred_real, _, _, _ = D(real_imgs)\n",
    "        pred_fake, _, _, _ = D(fake_imgs.detach())\n",
    "        loss_D_real = adversarial_loss(pred_real, real) # real_imgs는 D가 1(real)로 분류하도록 D 학습 \n",
    "        loss_D_fake = adversarial_loss(pred_fake, fake) # fake_imgs는 D가 0(fake)로 분류하도록 D 학습\n",
    "        loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "        \n",
    "        loss_D.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # ------------------\n",
    "        # Information Loss\n",
    "        # ------------------\n",
    "        optimizer_info.zero_grad()\n",
    "        code_P, code_G, latent = M(encoded.clone())\n",
    "        predicted = P(code_P)\n",
    "        fake_imgs = G(real_labels, code_P, code_G, latent)\n",
    "        _, pred_label, pred_code_P, pred_code_G = D(fake_imgs) # D라고 해놨지만 Q head의 출력임\n",
    "        # pred_label = Variable(LongTensor(to_np(pred_label))).to(device)\n",
    "        # labels = Variable(LongTensor(to_np(labels))).to(device)\n",
    "        \n",
    "        loss_info_disc = lambda_disc * discrete_loss(pred_label, real_labels) # 실제 레이블 예측(이산 CELoss)\n",
    "        loss_info_code_P = lambda_code * code_loss(pred_code_P.clone(), code_P.clone()) # code_P 예측 (연속 MSELoss\n",
    "        loss_info_code_G = lambda_code * code_loss(pred_code_G, code_G) # code_G 예측(연속 MSELoss)\n",
    "        loss_info =  loss_info_disc + loss_info_code_P + loss_info_code_G\n",
    "        \n",
    "        loss_info.backward(retain_graph=True)\n",
    "        optimizer_info.step()\n",
    "            \n",
    "    # --------------\n",
    "    # Log Progress\n",
    "    # --------------\n",
    "    M.eval() ; P.eval() ; G.eval() ; D.eval()\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for test_image, test_label in test_loader:\n",
    "            batch_size = test_image.shape[0]\n",
    "            test_image = test_image.to(device)\n",
    "            test_label = test_label.to(device)\n",
    "            code_P, code_G, latent = M(E(test_image).clone())\n",
    "            output = P(code_P).to(device)\n",
    "            prediction = output.max(1,keepdim=True)[1].to(device)\n",
    "            correct += prediction.eq(test_label.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_accuracy = 100*correct / len(test_loader.dataset)\n",
    "    if epoch % 5 == 0:\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [acc : %.2f] [P : %.4f] [D : %.4f] [G : %.4f] [info: %.4f] [code P: %.4f] [code G: %.4f] [time: %.1f]\"\n",
    "            % (epoch, args['Epochs'],\n",
    "            test_accuracy,\n",
    "            #    i, len(train_loader),\n",
    "            loss_P.item(),\n",
    "            loss_D.item(), \n",
    "            loss_G.item(), \n",
    "            loss_info.item(),\n",
    "            loss_info_code_P.item(),\n",
    "            loss_info_code_G.item(),\n",
    "            time.time()-start)\n",
    "        )\n",
    "    \n",
    "    sample, grid = sample_image(epoch=epoch)\n",
    "    \n",
    "    image_writer.add_image('sample', grid, epoch)\n",
    "    loss_writer.add_scalar('loss_P', loss_P.item(), epoch)\n",
    "    loss_writer.add_scalar('loss_D', loss_D.item(), epoch)\n",
    "    loss_writer.add_scalar('loss_G', loss_G.item(), epoch)\n",
    "    loss_writer.add_scalar('loss_info', loss_info.item(), epoch)\n",
    "    loss_writer.add_scalar('loss_info_d', loss_info_disc.item(), epoch)\n",
    "    loss_writer.add_scalar('loss_info_code_P', loss_info_code_P.item(), epoch)\n",
    "    loss_writer.add_scalar('loss_info_code_G', loss_info_code_G.item(), epoch)\n",
    "    loss_writer.add_scalar('accuracy', test_accuracy, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_one = x[25].unsqueeze(0).clone()\n",
    "image_seven = x[10].unsqueeze(0).clone()\n",
    "sample_image(300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (imgs,labels) in enumerate(train_loader):\n",
    "    batch_size = imgs.shape[0]\n",
    "    real = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False).to(device)\n",
    "    fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False).to(device)\n",
    "    \n",
    "    real_imgs = Variable(imgs.type(FloatTensor)).to(device)\n",
    "    real_labels = to_discrete(labels.numpy(), args['n_classes']).to(device)\n",
    "    labels = labels.to(device)\n",
    "    encoded = E(real_imgs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "997969026bb0563814df38f7ef8affc802fa04c571d985f23020a609d23c35a7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('opcode': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
