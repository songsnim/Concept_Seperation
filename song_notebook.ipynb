{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 날짜 : 2022-02-15\n",
      "GPU device : cuda:2\n",
      "image.size() =  torch.Size([128, 1, 32, 32]) \ttype torch.FloatTensor\n",
      "label.size() =  torch.Size([128]) \ttype torch.LongTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "args = {\n",
    "        'GPU_NUM' : 2,\n",
    "        'Epochs' : 200,\n",
    "        'batch_size' : 128,\n",
    "        'lr' : 0.0002,\n",
    "        'b1' : 0.5,\n",
    "        'b2' : 0.999,\n",
    "        'latent_dim' : 62,\n",
    "        'code_dim' : 2,\n",
    "        'n_classes' : 2,\n",
    "        'img_size' : 32,\n",
    "        'channels' : 1,\n",
    "        'sample_interval' : 400\n",
    "        }\n",
    "\n",
    "device = torch.device('cuda:{}'.format(args['GPU_NUM']) if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "today = datetime.date.today()\n",
    "print('오늘 날짜 :',today)\n",
    "print('GPU device :', device)\n",
    "\n",
    "my_transform =transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                transforms.Resize(args['img_size']), \n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "train_data = ImageFolder('MNIST/classes/binary/train', transform = my_transform)\n",
    "test_data = ImageFolder('MNIST/classes/binary/test', transform = my_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=args['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=args['batch_size'], shuffle=True)\n",
    "\n",
    "pltsize = 1\n",
    "plt.figure(figsize=(10*pltsize,pltsize))\n",
    "for image, label in train_loader:\n",
    "    print('image.size() = ',image.size(), '\\ttype', image.type())\n",
    "    print('label.size() = ', label.size(), '\\ttype', label.type())\n",
    "    break\n",
    "\n",
    "plt.figure(figsize=(10*pltsize,pltsize))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image[i,:,:,:].permute(1,2,0), cmap=\"gray\")\n",
    "    plt.title('class '+ str(label[i].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(tensor):\n",
    "    return tensor.cpu().detach().numpy()\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    \"\"\" Conv layer는 mean이 0, std가 0.02인 가우시안 분포로 weight init\n",
    "        BatchNorm은 mean이 1, std가 0.02인 가우시안 분포로 weight init\n",
    "        Bias term은 전부 0으로 초기화\n",
    "    Args:\n",
    "        m ([model]): 학습하려는 모델\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "def to_discrete(y, num_columns):\n",
    "    \"\"\" onehot encoding\n",
    "        (batch_size,)가 shape인 label이 있으면, (64,num_columns)인 zeros 행렬을 생성하고,\n",
    "        (batch_size,)의 label vector element 값의 index만 1로 바꿔서 one-hot encoding함\n",
    "    Args:\n",
    "        y : 어떤 array (y.shape[0]는 batch_size로 보면 됨)\n",
    "        num_columns : num_classes\n",
    "    \"\"\"\n",
    "    y_disc = np.zeros((y.shape[0], num_columns))\n",
    "    y_disc[range(y.shape[0]), y] = 1.0 # one-hot encoding()\n",
    "\n",
    "    return Variable(FloatTensor(y_disc))\n",
    "\n",
    "def sample_image(epoch):\n",
    "    folder_path = datetime.date.today()\n",
    "    os.makedirs('samples/{}_{}'.format(folder_path,args['description']), exist_ok=True)\n",
    "\n",
    "    inter = torch.linspace(-2,2,10).unsqueeze(1).to(device)\n",
    "   \n",
    "    # one !!\n",
    "    true_p, true_g, latent = M(E(image_one))\n",
    "    inter_p = inter\n",
    "    inter_g0 = torch.cat([inter, true_g[0, 1].unsqueeze(0).repeat(10,1)], dim=-1).to(device)\n",
    "    inter_g1 = torch.cat([true_g[0, 0].unsqueeze(0).repeat(10,1), inter], dim=-1).to(device)\n",
    "    label_one = torch.tensor([1,0], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    one_repeat = label_one.repeat(10,1).to(device)\n",
    "    one_latent_repeat = latent.repeat(10,1).to(device)\n",
    "    one_p = G(one_repeat, inter_p, true_g.repeat(10,1), one_latent_repeat)\n",
    "    one_g1 = G(one_repeat, true_p.repeat(10,1), inter_g0, one_latent_repeat)\n",
    "    one_g2 = G(one_repeat, true_p.repeat(10,1), inter_g1, one_latent_repeat)\n",
    "    sample_one = torch.cat([image_one,one_p, image_one,one_g1, image_one,one_g2], dim=0).to(device)\n",
    "\n",
    "    # seven!!!\n",
    "    true_p, true_g, latent = M(E(image_seven))\n",
    "    inter_p = inter\n",
    "    inter_g0 = torch.cat([inter, true_g[0, 1].unsqueeze(0).repeat(10,1)], dim=-1).to(device)\n",
    "    inter_g1 = torch.cat([true_g[0, 0].unsqueeze(0).repeat(10,1), inter], dim=-1).to(device)\n",
    "    label_seven = torch.tensor([0,1], dtype=torch.int64, device=device).unsqueeze(0)\n",
    "    seven_repeat = label_seven.repeat(10,1).to(device)\n",
    "    seven_latent_repeat = latent.repeat(10,1).to(device)\n",
    "    seven_p = G(seven_repeat, inter_p, true_g.repeat(10,1), seven_latent_repeat)\n",
    "    seven_g1 = G(seven_repeat, true_p.repeat(10,1), inter_g0, seven_latent_repeat)\n",
    "    seven_g2 = G(seven_repeat, true_p.repeat(10,1), inter_g1, seven_latent_repeat)\n",
    "    sample_seven = torch.cat([image_seven,seven_p,image_seven,seven_g1, image_seven,seven_g2], dim=0).to(device)\n",
    "    \n",
    "    sample = torch.cat([sample_one,sample_seven], dim=0).to(device)\n",
    "        \n",
    "    grid = torchvision.utils.make_grid(sample, nrow=11, normalize=True)\n",
    "    \n",
    "    save_image(grid, 'samples/{}_{}/grid_{}.png'.format(folder_path,args['description'],epoch))\n",
    "    \n",
    "    return sample, grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction criterion\n",
    "predict_loss = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# adversarial criterion\n",
    "adversarial_loss = nn.MSELoss().to(device)\n",
    "fidelity_loss = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# reconstruction criterion\n",
    "recon_loss = nn.L1Loss().to(device)\n",
    "recon_loss_E = nn.L1Loss().to(device)\n",
    "fidelity_loss = nn.KLDivLoss(reduction='batchmean').to(device)\n",
    "\n",
    "# Info criterion\n",
    "code_P_loss = nn.MSELoss().to(device)\n",
    "code_G_loss = nn.MSELoss().to(device)\n",
    "\n",
    "lambda_disc = 1\n",
    "lambda_code = 0.5\n",
    "\n",
    "from pretrain_ResNet import ResNet_3232\n",
    "pretrained_resnet = ResNet_3232(channels=1, num_classes=2).to(device)\n",
    "pretrained_resnet.load_state_dict(torch.load('pretrained_model/ResNet_3232_parameters_1_7.pt'))\n",
    "\n",
    "args['code_P_dim'] = 1\n",
    "args['code_G_dim'] = 2\n",
    "args['latent_dim'] = 32\n",
    "args['reduced_dim'] = args['code_P_dim'] + args['code_G_dim'] + args['latent_dim'] # 35\n",
    "\n",
    "# from StyleInfoGAN import *\n",
    "from Networks import *\n",
    "E = nn.Sequential(*(list(pretrained_resnet.children())[:5])).to(device)\n",
    "M = Mapper(args).to(device)\n",
    "P = Predictor(args).to(device)\n",
    "G = Generator(args).to(device)\n",
    "D = Discriminator(args).to(device)\n",
    "\n",
    "\n",
    "M.apply(weights_init_normal)\n",
    "P.apply(weights_init_normal)\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "optimizer_P = torch.optim.Adam(itertools.chain(M.parameters(),P.parameters()), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(M.parameters(),G.parameters()), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "optimizer_D = torch.optim.Adam(itertools.chain(M.parameters(),D.parameters()), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "optimizer_info = torch.optim.Adam(itertools.chain(M.parameters(),G.parameters(), D.parameters()), lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "# optimizer_info_P = torch.optim.Adam(itertools.chain(M.parameters(), P.parameters(), G.parameters(), D.parameters()), \n",
    "#                                     lr=args['lr'], betas=(args['b1'], args['b2']))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if (device == 'cuda:{}'.format(args['GPU_NUM'])) else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if (device == 'cuda:{}'.format(args['GPU_NUM'])) else torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x, y in train_loader:\n",
    "#     x = x.clone().to(device)\n",
    "#     y = y.clone().to(device)\n",
    "#     break\n",
    "\n",
    "# plt.figure(figsize=(15,12))\n",
    "# for idx in range(20):\n",
    "#     plt.subplot(4,5,idx+1)\n",
    "#     plt.axis('off')\n",
    "#     plt.title('idx={}, label={}'.format(idx, y[idx].item()))\n",
    "#     plt.imshow(to_np(x[idx].permute(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts!\n",
      "[Epoch 0/200] [acc : 99.62] [P : 0.0755] [D : 0.0185] [G : 1.6311] [info: 0.0130] [code P: 0.0121] [code G: 0.0138] [time: 20.6]\n",
      "[Epoch 5/200] [acc : 99.62] [P : 0.0035] [D : 0.2156] [G : 0.6723] [info: 0.0072] [code P: 0.0060] [code G: 0.0084] [time: 159.1]\n",
      "[Epoch 10/200] [acc : 99.65] [P : 0.0011] [D : 0.2610] [G : 0.5761] [info: 0.0079] [code P: 0.0076] [code G: 0.0082] [time: 378.5]\n",
      "[Epoch 15/200] [acc : 99.65] [P : 0.1006] [D : 0.2767] [G : 0.6072] [info: 0.0158] [code P: 0.0239] [code G: 0.0078] [time: 507.9]\n",
      "[Epoch 20/200] [acc : 99.65] [P : 0.0012] [D : 0.2403] [G : 0.7123] [info: 0.0148] [code P: 0.0089] [code G: 0.0207] [time: 620.1]\n",
      "[Epoch 25/200] [acc : 99.69] [P : 0.0003] [D : 0.2338] [G : 0.5496] [info: 0.0052] [code P: 0.0058] [code G: 0.0046] [time: 747.9]\n",
      "[Epoch 30/200] [acc : 99.65] [P : 0.0214] [D : 0.2243] [G : 0.4949] [info: 0.0084] [code P: 0.0069] [code G: 0.0099] [time: 948.5]\n",
      "[Epoch 35/200] [acc : 99.73] [P : 0.0001] [D : 0.2706] [G : 0.4835] [info: 0.0052] [code P: 0.0058] [code G: 0.0046] [time: 1190.2]\n",
      "[Epoch 40/200] [acc : 99.69] [P : 0.0002] [D : 0.2397] [G : 0.4651] [info: 0.0068] [code P: 0.0077] [code G: 0.0060] [time: 1344.6]\n",
      "[Epoch 45/200] [acc : 48.23] [P : 0.0001] [D : 0.2675] [G : 0.5412] [info: 0.0120] [code P: 0.0130] [code G: 0.0109] [time: 1463.3]\n",
      "[Epoch 50/200] [acc : 99.69] [P : 0.0034] [D : 0.2527] [G : 0.5300] [info: 0.0094] [code P: 0.0101] [code G: 0.0087] [time: 1566.7]\n",
      "[Epoch 55/200] [acc : 99.69] [P : 0.0000] [D : 0.2475] [G : 0.5609] [info: 0.0031] [code P: 0.0034] [code G: 0.0027] [time: 1677.1]\n",
      "[Epoch 60/200] [acc : 99.81] [P : 0.0000] [D : 0.2130] [G : 0.5159] [info: 0.0075] [code P: 0.0071] [code G: 0.0079] [time: 1830.9]\n",
      "[Epoch 65/200] [acc : 99.69] [P : 0.0007] [D : 0.2314] [G : 0.5270] [info: 0.0073] [code P: 0.0062] [code G: 0.0085] [time: 1953.9]\n",
      "[Epoch 70/200] [acc : 99.69] [P : 0.1265] [D : 0.2147] [G : 0.7214] [info: 0.0085] [code P: 0.0097] [code G: 0.0073] [time: 2052.4]\n",
      "[Epoch 75/200] [acc : 99.65] [P : 0.0000] [D : 0.2958] [G : 0.5329] [info: 0.0048] [code P: 0.0050] [code G: 0.0045] [time: 2220.5]\n",
      "[Epoch 80/200] [acc : 99.69] [P : 0.0001] [D : 0.1567] [G : 0.5703] [info: 0.0072] [code P: 0.0077] [code G: 0.0068] [time: 2394.4]\n",
      "[Epoch 85/200] [acc : 99.65] [P : 0.0000] [D : 0.2417] [G : 0.4668] [info: 0.0043] [code P: 0.0037] [code G: 0.0048] [time: 2567.4]\n",
      "[Epoch 90/200] [acc : 99.69] [P : 0.0570] [D : 0.2451] [G : 0.5554] [info: 0.0147] [code P: 0.0150] [code G: 0.0144] [time: 2669.4]\n",
      "[Epoch 95/200] [acc : 99.69] [P : 0.0043] [D : 0.2474] [G : 0.5689] [info: 0.0048] [code P: 0.0046] [code G: 0.0051] [time: 2778.9]\n",
      "[Epoch 100/200] [acc : 99.77] [P : 0.0001] [D : 0.1798] [G : 0.4093] [info: 0.0059] [code P: 0.0063] [code G: 0.0054] [time: 2890.9]\n",
      "[Epoch 105/200] [acc : 99.65] [P : 0.0001] [D : 0.2502] [G : 0.5371] [info: 0.0044] [code P: 0.0032] [code G: 0.0056] [time: 2992.6]\n",
      "[Epoch 110/200] [acc : 99.58] [P : 0.0001] [D : 0.2321] [G : 0.5332] [info: 0.0029] [code P: 0.0031] [code G: 0.0026] [time: 3087.7]\n",
      "[Epoch 115/200] [acc : 99.69] [P : 0.0000] [D : 0.2420] [G : 0.5292] [info: 0.0043] [code P: 0.0032] [code G: 0.0053] [time: 3239.2]\n",
      "[Epoch 120/200] [acc : 99.58] [P : 0.0427] [D : 0.2840] [G : 0.4780] [info: 0.0086] [code P: 0.0114] [code G: 0.0059] [time: 3390.3]\n",
      "[Epoch 125/200] [acc : 99.65] [P : 0.0001] [D : 0.3310] [G : 0.5259] [info: 0.0042] [code P: 0.0034] [code G: 0.0049] [time: 3506.3]\n",
      "[Epoch 130/200] [acc : 99.77] [P : 0.0000] [D : 0.2949] [G : 0.5742] [info: 0.0053] [code P: 0.0036] [code G: 0.0071] [time: 3598.1]\n",
      "[Epoch 135/200] [acc : 99.69] [P : 0.0000] [D : 0.1683] [G : 0.6977] [info: 0.0047] [code P: 0.0040] [code G: 0.0053] [time: 3691.6]\n",
      "[Epoch 140/200] [acc : 99.69] [P : 0.0000] [D : 0.3050] [G : 0.5978] [info: 0.0059] [code P: 0.0058] [code G: 0.0061] [time: 3800.5]\n",
      "[Epoch 145/200] [acc : 99.69] [P : 0.0011] [D : 0.3294] [G : 0.7591] [info: 0.0075] [code P: 0.0061] [code G: 0.0088] [time: 3973.4]\n",
      "[Epoch 150/200] [acc : 99.62] [P : 0.0005] [D : 0.1197] [G : 0.7705] [info: 0.0153] [code P: 0.0133] [code G: 0.0172] [time: 4147.0]\n",
      "[Epoch 155/200] [acc : 99.77] [P : 0.0000] [D : 0.2852] [G : 0.9909] [info: 0.0154] [code P: 0.0126] [code G: 0.0182] [time: 4291.4]\n",
      "[Epoch 160/200] [acc : 99.69] [P : 0.0003] [D : 0.2212] [G : 0.4707] [info: 0.0089] [code P: 0.0082] [code G: 0.0096] [time: 4388.8]\n",
      "[Epoch 165/200] [acc : 99.65] [P : 0.0000] [D : 0.1759] [G : 0.6355] [info: 0.0046] [code P: 0.0038] [code G: 0.0053] [time: 4492.7]\n"
     ]
    }
   ],
   "source": [
    "image_one = x[2].unsqueeze(0).clone()\n",
    "image_seven = x[8].unsqueeze(0).clone()\n",
    "\n",
    "import torchvision\n",
    "from torch.utils import tensorboard\n",
    "\n",
    "args['description'] = 'L1 loss and KL loss for generator'\n",
    "today = datetime.date.today()\n",
    "loss_writer = tensorboard.SummaryWriter('logs/MNIST/loss/{}_{}'.format(today,args['description']))\n",
    "image_writer = tensorboard.SummaryWriter('logs/MNIST/image/{}_{}'.format(today,args['description']))\n",
    "\n",
    "start = time.time() ; print('Training starts!')\n",
    "\n",
    "for epoch in range(args['Epochs']):\n",
    "    M.train() ; P.train() ; G.train() ; D.train()\n",
    "    for i, (imgs,labels) in enumerate(train_loader):\n",
    "        batch_size = imgs.shape[0]\n",
    "        real = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False).to(device)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False).to(device)\n",
    "        \n",
    "        real_imgs = Variable(imgs.type(FloatTensor)).to(device)\n",
    "        real_labels = to_discrete(labels.numpy(), args['n_classes']).to(device)\n",
    "        labels = labels.to(device)\n",
    "        encoded = E(real_imgs)        \n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Predictor\n",
    "        # -----------------\n",
    "        optimizer_P.zero_grad()\n",
    "        code_P, code_G, latent = M(encoded.clone())\n",
    "        predicted= P(code_P)\n",
    "        loss_P = predict_loss(predicted, real_labels)\n",
    "\n",
    "        loss_P.backward(retain_graph=True)\n",
    "        optimizer_P.step()\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        code_P, code_G, latent = M(encoded.clone())\n",
    "        real_M_output = torch.cat([code_P, code_G, latent], dim=1)\n",
    "        fake_imgs = G(real_labels, code_P, code_G, latent)\n",
    "        code_P_fake, code_G_fake, latent_fake = M(E(fake_imgs).clone())\n",
    "        fake_M_output = torch.cat([code_P_fake, code_G_fake, latent_fake], dim=1)\n",
    "        reality, _, _, _ = D(fake_imgs)\n",
    "        \n",
    "        loss_adv = adversarial_loss(reality, real) # fake_imgs의 분류(D) 결과가 최대한 1(real)로 분류되도록 G 학습\n",
    "        loss_recon = recon_loss(real_imgs, fake_imgs)\n",
    "        loss_recon_E = recon_loss_E(E(fake_imgs), encoded)\n",
    "        loss_fidelity = fidelity_loss((P(code_P_fake)), torch.exp(P(code_P)))\n",
    "        # alpha = 0.1\n",
    "        # loss_G = (alpha*loss_adv + (1-alpha)*loss_recon)\n",
    "        loss_G = loss_adv + (loss_recon + loss_recon_E) + loss_fidelity\n",
    "        loss_G.backward(retain_graph=True)\n",
    "        optimizer_G.step()\n",
    "    \n",
    "        # -----------------\n",
    "        #  Train Discriminator\n",
    "        # -----------------\n",
    "        optimizer_D.zero_grad()\n",
    "        code_P, code_G, latent = M(encoded.clone())\n",
    "        # real or fake pred score\n",
    "        pred_real, _, _, _ = D(real_imgs.detach())\n",
    "        pred_fake, _, _, _ = D(fake_imgs.detach())\n",
    "        loss_D_real = adversarial_loss(pred_real, real) # real_imgs는 D가 1(real)로 분류하도록 D 학습\n",
    "        loss_D_fake = adversarial_loss(pred_fake, fake) # fake_imgs는 D가 0(fake)로 분류하도록 D 학습\n",
    "        loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "        \n",
    "        loss_D.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # ------------------\n",
    "        # Information Loss\n",
    "        # ------------------\n",
    "        optimizer_info.zero_grad()\n",
    "        code_P, code_G, latent = M(encoded.clone())\n",
    "        predicted = P(code_P)\n",
    "        fake_imgs = G(real_labels, code_P, code_G, latent)\n",
    "        _, _, pred_code_P, pred_code_G = D(fake_imgs) # D라고 해놨지만 Q head의 출력임\n",
    "        \n",
    "        loss_info_code_P = lambda_code * code_P_loss(pred_code_P.clone(), code_P.clone()) # code_P 예측 (연속 MSELoss)\n",
    "        loss_info_code_G = lambda_code * code_G_loss(pred_code_G, code_G) # code_G 예측(연속 MSELoss)\n",
    "        loss_info =  lambda_code * (loss_info_code_P + loss_info_code_G)\n",
    "        \n",
    "        loss_info.backward(retain_graph=True)\n",
    "        optimizer_info.step()\n",
    "            \n",
    "    # --------------\n",
    "    # Log Progress\n",
    "    # --------------\n",
    "    M.eval() ; P.eval() ; G.eval() ; D.eval()\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for test_image, test_label in test_loader:\n",
    "            batch_size = test_image.shape[0]\n",
    "            test_image = test_image.to(device)\n",
    "            test_label = test_label.to(device)\n",
    "            code_P, code_G, latent = M(E(test_image).clone())\n",
    "            output = P(code_P).to(device)\n",
    "            prediction = output.max(1,keepdim=True)[1].to(device)\n",
    "            correct += prediction.eq(test_label.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_accuracy = 100*correct / len(test_loader.dataset)\n",
    "    if epoch % 5 == 0:\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [acc : %.2f] [P : %.4f] [D : %.4f] [G : %.4f] [info: %.4f] [code P: %.4f] [code G: %.4f] [time: %.1f]\"\n",
    "            % (epoch, args['Epochs'],\n",
    "            test_accuracy,\n",
    "            #    i, len(train_loader),\n",
    "            loss_P.item(),\n",
    "            loss_D.item(), \n",
    "            loss_G.item(), \n",
    "            loss_info.item(),\n",
    "            loss_info_code_P.item(),\n",
    "            loss_info_code_G.item(),\n",
    "            time.time()-start)\n",
    "        )\n",
    "    \n",
    "    sample, grid = sample_image(epoch=epoch)\n",
    "    \n",
    "    image_writer.add_image('sample', grid, epoch)\n",
    "    # loss_writer.add_scalar('loss_P', loss_P.item(), epoch)\n",
    "    loss_writer.add_scalar('loss_D', loss_D.item(), epoch)\n",
    "    loss_writer.add_scalar('loss_G', loss_G.item(), epoch)\n",
    "    loss_writer.add_scalar('loss_adv', loss_adv.item(), epoch)\n",
    "    loss_writer.add_scalar('loss_recon', loss_recon.item(), epoch)\n",
    "    loss_writer.add_scalar('loss_recon_E', loss_recon_E.item(), epoch)\n",
    "    loss_writer.add_scalar('loss_fidelity', loss_fidelity.item(), epoch)\n",
    "    loss_writer.add_scalar('loss_info', loss_info.item(), epoch)\n",
    "    loss_writer.add_scalar('loss_info_code_P', loss_info_code_P.item(), epoch)\n",
    "    loss_writer.add_scalar('loss_info_code_G', loss_info_code_G.item(), epoch)\n",
    "    loss_writer.add_scalar('accuracy', test_accuracy, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_one = x[25].unsqueeze(0).clone()\n",
    "image_seven = x[10].unsqueeze(0).clone()\n",
    "sample_image(300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (imgs,labels) in enumerate(train_loader):\n",
    "    batch_size = imgs.shape[0]\n",
    "    real = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False).to(device)\n",
    "    fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False).to(device)\n",
    "    \n",
    "    real_imgs = Variable(imgs.type(FloatTensor)).to(device)\n",
    "    real_labels = to_discrete(labels.numpy(), args['n_classes']).to(device)\n",
    "    labels = labels.to(device)\n",
    "    encoded = E(real_imgs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "997969026bb0563814df38f7ef8affc802fa04c571d985f23020a609d23c35a7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('opcode': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
